[["index.html", "Example workflow of amplicon sequence data analysis 1 Introduction 1.1 Setting up your environment 1.2 Download fastqs", " Example workflow of amplicon sequence data analysis Karine Villeneuve 2024-09-17 1 Introduction This guide is intended to walk the user through the typical R workflow for processing raw amplicon sequencing data from paired end Illumina Miseq data into a table of exact amplicon sequence variants (ASVs) present in each sample. Please note that many other extensive documentation and tutorial pages are available for packages used in this workflow, notably DADA2 and phyloseq. Therefore, for issues or for further questions about certain functions I always recommend consulting the available R documentation and if available the relevant github issues sites for answers. This guide assumes the following : The user has access to one of the server from the Lazar lab (Orion, Hercules, Ulysse). The user has an active VPN access. The paired-end fastq files from Illumina Miseq sequencing were transferred to the user’s home directory of her/his server. Please see section Download fastqs for more details. The user has followed the Introduction to linux guide (available on the lab’s TEAM) and is comfortable with basic command line functions such as : listing files inside a current directory (ls) ; moving from one directory to the other (cd) ; creating new directory (mkdir) ; and moving / copying (mv/cp) files from one directory to the other. 1.1 Setting up your environment Keeping your files organized is a skill that has a high long-term payoff. As you are in the thick of an analysis, you may underestimate how many files/folders you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped, which can ultimately lead to significantly slower research progress. Furthermore, one must keep in mind that someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. While there’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution, below we propose a simple organizational scheme which is project-oriented, maintainable and ultimately follows consistent patterns for amplicon sequence analysis. Please note that the proposed workflow assumes such organization. ┌─ ~ -------------------------------- Your home directory │ ├── chapter1_16S_diversity ------ Project with a short but meaningfull name with │ │ a second level domain-specific organization (if applicable). │ ├── archaea │ ├── eukaryotes │ ├── bacteria ---------------- Third level domain-specific organization : │ ├── data -------------- final datasets generated by scripts to be used for interpretation ; │ ├── int_data ----------- intermediate data to be used by other script (DADA2 output, rarefied data, etc.) ; │ ├── figures ------------ figures generated by scripts ; │ ├── raw_data ---------- raw files (i.e. fastq files generated by sequencing, raw metadata table, etc.) ; and │ ├── BAC_sample-1_R1.fastq.gz │ ├── BAC_sample-1_R2.fastq.gz │ ├── BAC_sample-2_R1.fastq.gz │ ├── BAC_sample-2_R2.fastq.gz │ └── ... │ └── scripts ----------- executable and scripts. │ ├── bac_DADA2.rmd │ ├── bac_rarefy.rmd │ ├── bac_alpha_div.rmd │ ├── bac_stacked_barchart.rmd │ └── ... │ ├── data ------------------ Project-specific final datasets for publication │ ├── figures ----------------- Project-specific final figures for publication │ └── scripts ----------------- Project-specific scripts (only if applicable, i.e. multi-domain, explanatory variables, etc.) └────────────────────────────────────────────────────────────────── Further reading about organizing files and foldes : Organizing your project by the Johns Hopkins Data Science Lab A Quick Guide to Organizing Computational Biology Projects by William Stafford Noble, 2009 Reddit post Organizing your data by The Max Delbrück Center 1.2 Download fastqs How to download fastqs files from Illumina BaseSpace Sequence Hub Open the link found in the email sent by Geneviève Bourret. If this is your first time downloading your fastqs create a new BaseSpace Sequence Hub account using the email address to which the email from Geneviève was addressed (normally this would be your UQAM’s email) On the pop-up window informing you that the CERMO-FC has shared the following item with you click ACCEPT. Click on the PROJECTS tab in the upper section of the page. Select your project and then click on the second round logo from the left which looks like a blank page and in the drop-down menu select DOWNLOAD then PROJECT. If required, download the Illumina Basespace downloader by clicking INSTALL DOWNLOAD and follow the instructions. Otherwise simply click DOWNLOAD to begin downloading your fastqs. Once the download is complete you will find inside the folder a folder for each of your sample inside which the forward and reverse read are both found in another folder. Instead of going into each folder individually and copying the fastqs manually we can use the terminal to do the job for us. From a new local terminal window navigate to the folder containing all the folders and execute the following command after having modified /path/to/directory/where/to/move/fastqs to the actual path where you wish to move your fastqs. find ./ -name &quot;*.gz&quot; -exec cp -prv &quot;{}&quot; &quot;/path/to/directory/where/to/move/fastqs&quot; &quot;;&quot; Finally you can transfer your fastqs to your assigned server using any File Transfer Protocol (FTP) clients (such as FileZilla or Cyberduck) or using the SCP (secure copy) command-line utility. For SCP you can copy an entire folder by opening a new local terminal window and navigating to the directory containing the folder with the fastqs. From that directory execute the following command. You will then be asked to enter the password for your user on the server. scp -r name_of_foler_with_fastqs username@server.bio.uqam.ca:/path/to/copy/folder "],["dada2-pipeline.html", "2 DADA2 pipeline 2.1 General workflow 2.2 Complete code 2.3 Forward reads only", " 2 DADA2 pipeline pre, code {white-space:pre !important; overflow-x:auto} This general workflow presents the typical commands used to process amplicon sequencing data. Please note that for some steps the commands will vary based on which Kingdom (Bacteria, Archaea or Eukaryote) the data being processed belongs to. The first section (General workflow) generally describes and breakdowns each steps of the analysis. In the second section (Complete code) the reader will find a single chunk of code which she/he can copy-paste into a new rmarkdown document and execute each chunk of code. Finally, the third section (Forward reads only) also contains a single chunk of code to use only with archaeal sequences when the quality of the reverse read for is of too poor quality to allow the merging of forward and reverse read and thus only the forward reads are processed. 2.1 General workflow 2.1.1 Getting ready Load required libraries library(dada2) library(decontam) library(phyloseq) library(DECIPHER) library(phangorn) The first step is to define where the fastq files are located. Please Modify this path accordingly. For more information on how to organize your files and folders on your server please see section Setting up your environment. To validate that we are in the correct folder we then use the command list.files to print out all files contained in the folder previously defined. path = &quot;~/project/domain/raw_data&quot; list.files(path) We then extract each sample name from the forward and reverse fastq files using some string manipulation assuming the name of our files respect the following format : Forward reads : sample-name_domain_xxx_L001_R1_001.fastq Reverse reads : sample-name_domain_xxx_L001_R2_001.fastq fnFs = sort(list.files(path, pattern=&quot;_R1_001.fastq&quot;, full.names = TRUE)) fnRs = sort(list.files(path, pattern=&quot;_R2_001.fastq&quot;, full.names = TRUE)) sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) 2.1.2 Inspect quality In order to inspect the read quality profiles we use the command plotQualityProfile to plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq file. plotQualityProfile(fnFs, aggregate=TRUE) plotQualityProfile(fnRs, aggregate=TRUE) In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line). The reverse reads are generally of significantly worse quality, especially at the end, which is common in Illumina sequencing. 2.1.3 Filter and trim sequences Before trimming we assign the filenames for the filtered fastq.gz files and place filtered files in the created filtered subdirectory. The command trimleft is used to remove the primers (based on primer length) and truncLento trim the reads based on where the average quality begins to crash on the previously generated graphs. Nucleotides after the specified position will be removed. For Bacterias Lenght of primer B341F (CCT ACG GGA GGC AGC AG) : 18 nucleotides Lenght of primer B785R (GAC TAC HVG GGT ATC TAA TCC): 21 nucleotides For Archaea Lenght of primer A340F (CCC TAC GGG GYG CAS CAG) : 18 nucleotides Lenght of primer A915R (GTG CTC CCC CGC CAA TTC CT) : 20 nucleotides For Eukaryotes Lenght of primer E960F (GGC TTA ATT TGA CTC AAC RCG) : 21 nucleotides Lenght of primer NSR1438R (GGC TTA ATT TGA CTC AAC RCG) : 21 nucleotides filtFs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_F_filt.fastq.gz&quot;)) filtRs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_R_filt.fastq.gz&quot;)) names(filtFs) = sample.names names(filtRs) = sample.names # For Bacteria out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,21), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) # For Archaea out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,20), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) # For Eukaryotes out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(21,21), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) 2.1.4 Learn error rates The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors). Please note that for some reason multithread=TRUE causes problem for users on a Microsoft Windows operating system and should therefore set this parameter to FALSE. Such modification should be made for all other instances where parameter multithread is specified # Learn error rates for forward and reverse reads errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE) errR = learnErrors(filtRs, multithread=TRUE, randomize=TRUE) # visualize the estimated error rates, as a sanity check if nothing else plotErrors(errF, nominalQ=TRUE) # Apply the core sample inference algorithm to the filtered and trimmed sequence data dadaFs = dada(filtFs, err=errF, pool = &quot;pseudo&quot;, multithread=TRUE) dadaRs = dada(filtRs, err=errR, pool = &quot;pseudo&quot;, multithread=TRUE) 2.1.5 Merge paired reads We can now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments). # Merging the paired reads mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs) # Construct an amplicon sequence variant table (ASV) table (a higher-resolution version of the OTU table produced by traditional methods) seqtab = makeSequenceTable(mergers) #View dimension of your matrices dim(seqtab) # Inspect distribution of sequence lengths table(nchar(getSequences(seqtab))) 2.1.5.1 Remove chimeras Removing chimeras with the function removeBimeraDenovo. The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. # Remove chimeras seqtab.nochim = removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE) #View dimension of your matrices and proportion of non-chimeric sequences dim(seqtab.nochim) sum(seqtab.nochim)/sum(seqtab) 2.1.6 Track reads through pipeline As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline with the following commands. This is a great place to do a last sanity check. Outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification. getN = function(x) sum(getUniques(x)) track = cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nonchim&quot;) rownames(track) = sample.names track 2.1.7 Classify sequences The DADA2 package provides a native implementation of the naïve Bayesian classifier method to assign taxonomy to the sequence variants. The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments. The DADA2 team maintains DADA2-formatted reference fastas for the three most common 16S databases (Silva, RDP and GreenGenes) as well as additional trainings fastas suitable for protists and certain contributed specific environments The minimum bootstrap confidence for assigning a taxonomic level. Database for Procaryotes taxa = assignTaxonomy(seqtab.nochim, &quot;/home/16S_db/silva_nr99_v138.1_train_set.fa.gz&quot;, multithread=TRUE, tryRC=TRUE) Further classification for Archaeas The same silva database is used for the classification of Bacterias and Archaeas but we also use a custom database to further classify sequences which failed initial classification for Archaeas. For this we use the function IdTaxa from the DECIPHER package. # Extract sequences not identified to the phylum and domain taxint = subset(taxa, is.na(phylum)) taxide = subset(taxa, !(is.na(domain))) # View how many sequences dim(taxint) seqtabint =as.data.frame(seqtab.nochim) seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)] # Reclassify with custom database arc.cassandre load(&quot;/home/16S_db/arc.cassandre.trainingset.RData&quot;) dna = DNAStringSet(getSequences(seqtabint)) # Create a DNAStringSet from the ASVs ids = IdTaxa(dna, trainingSet, strand=&quot;both&quot;, processors=NULL, verbose=FALSE, threshold = 50) taxint = t(sapply(ids, function(x) { m = match(ranks, x$rank) taxa = x$taxon[m] taxa[startsWith(taxa, &quot;Unclassified_&quot;)] = NA taxa })) colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint) # Keep only sequences classified Archaea taxint =subset(as.data.frame(taxint), domain ==&quot;Archaea&quot;) # Swap previously classified sequences with SILVA to those classified using the custom and more precise database taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),] # Merge both tables taxa = rbind(taxide, as.data.frame(taxint)) seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)] Database for Eukaryotes As of Silva version 138, the official DADA2-formatted reference fastas are optimized for classification of Bacteria and Archaea, and are not suitable for classifying Eukaryotes and we therefore use PR2 database. Note that this database has different taxLevels than the DADA2 default. taxa = assignTaxonomy(seqtab.nochim, &quot;/home/16S_db/pr2_version_5.0.0_SSU_dada2.fasta.gz&quot;, multithread=TRUE, tryRC=TRUE, taxLevels = c(&quot;Kingdom&quot;,&quot;Supergroup&quot;,&quot;Division&quot;,&quot;Class&quot;,&quot;Order&quot;,&quot;Family&quot;,&quot;Genus&quot;,&quot;Species&quot;)) 2.1.8 Add highest identified taxonomic rank to unclassified ranks # transpose table taxid=data.frame(t(taxa)) # As a sanity check transform every cells to characters taxid[] = lapply(taxid, as.character) # Fills the NAs with the most recent non-NA value taxa2= tidyr::fill(taxid, colnames(taxid),.direction = &quot;down&quot;) # Paste Unclassified_ to the beginning of every cells taxa2= sapply(taxa2, function(x){paste0(&quot;Unclassified_&quot;, x)}) # Replace NAs to it&#39;s value from the table taxa2 taxid[is.na(taxid)] = taxa2[is.na(taxid)] # Transpose table again taxid = t(taxid) Finally we remove from the tax table and ASV matrix the ASVs not classified to our domain or Kingdom of interest. If applicable replace Bacteria for either Archaea or Eukaryotes. taxid=subset(as.data.frame(taxid), Kingdom ==&quot;Bacteria&quot;) seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)] 2.1.9 Removing contaminants The decontam package provides simple statistical methods to identify and visualize contaminating DNA features, allowing them to be removed and ultimately get a more accurate picture of the sampled communities to be constructed from marker-gene data. The package was designed to work with phyloseq objects from the phyloseq package. For this tutorial, the use of the decontam requires the two following : A table of the relative abundances of sequence features (columns) in each sample (rows). A metadata table where a defined set of “negative control” (samples in which sequencing was performed on blanks without any biological sample added) are identified as TRUE in a column called negative while other samples are identified as FALSE. We first load the metadata table where negative control are clearly identified and then generate a phyloseq object combining the table of relative abundances of sequences, the taxonomy table and the metadata table meta = read.table(&quot;metadata.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE) ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta)) The contaminant identification method used in this workflow is the prevalence method (presence/absence across samples). In this method, the prevalence of each sequence feature in true positive samples is compared to the prevalence in negative controls to identify contaminants. In the prevalence test there is a special value worth knowing, threshold=0.5, that will identify as contaminants all sequences that are are more prevalent in negative controls than in positive samples. contamdf.prev = isContaminant(ps, method=&quot;prevalence&quot;, neg=&quot;negative&quot;, threshold=0.5) # Get the count of TRUE contaminant vs FALSE table(contamdf.prev$contaminant) # Remove sequences identified as contaminant ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) # Remove negative control samples ps_decontam=subset_samples(ps.noncontam, !negative==&quot;TRUE&quot;) 2.1.10 Generate a phylogenetic tree Generating a phylogenetic tree is not mandatory for the downstream analysis unless the user plans on using any phylogeny based diversity metrics such as Unifrac. To build such tree we start by aligning the sequencing using the AlignSeqs function from package DECIPHER. # Extract sequences from phyloseq object asv_tab=as.data.frame(otu_table(ps_decontam)) seqs = getSequences(t(asv_tab)) names(seqs) = seqs # Aligning sequences seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20) # Set path to save the aligned sequence fastq file writeXStringSet(seq_align, file = &quot;~/project/domain/raw_data/align.fasta&quot;,format=&quot;fasta&quot;) We then used the aligned fastq file to generate the tree using the tool FastTree. As this tool is not an R library but rather a linux program the following commands must be executed from a terminal/command prompt. # Change directory to the one containing the align.fasta file cd ~/project/domain/raw_data/ # Execute fasttree algorithm on the align.fasta file and generate the file align_tree fasttree -nt -gtr align.fasta &gt; align_tree The generated phylogenetic tree needs to be rooted to calculate any phylogeny based diversity metrics. For this we use the function midpoint from package phangorn. # Load file align_tree Tree = ape::read.tree(file = &quot;~/project/domain/raw_data/align_tree&quot;) # Add midpoint to tree Tree.midpoint = phangorn::midpoint(Tree) Finally we add the rooted tree to our phyloseq object tree = phy_tree(Tree.midpoint) ps1 = merge_phyloseq(ps_decontam, tree) 2.1.11 Add DNA sequences dna = Biostrings::DNAStringSet(taxa_names(ps_decontam)) names(dna) = taxa_names(ps_decontam) ps1=merge_phyloseq(ps1, dna) 2.1.12 Shorten ASV name We shorten ASV name from complete sequence to ASV#. taxa_names(ps1) = paste0(&quot;ASV&quot;, seq(ntaxa(ps1))) 2.1.13 Save tables saving_path = &quot;~/project/domain/int_data&quot; write.csv(as.data.frame(as(tax_table(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_taxa.csv&quot;)) write.csv(as.data.frame(as(otu_table(ps1), &quot;matrix&quot;)),file = glue(&quot;{path}/raw_asv.csv&quot;)) write.csv(as.data.frame(as(sample_data(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_meta.csv&quot;)) tree.raw = phy_tree(ps1) ape::write.tree(tree.raw , file = glue(&quot;{path}/raw_tree.tree&quot;)) ps1 %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(glue(&quot;{path}/raw_refseq.fna&quot;), append=FALSE, compress=FALSE, compression_level=NA, format=&quot;fasta&quot;) 2.2 Complete code You can copy-paste the following block of code inside a new markdown document. Code-chunks will be automatically generated and you can use the far right button ( ▶ ) to execute all of the code inside each chunk. ```{r} # ----------- Load libraries ----------- library(dada2) library(decontam) library(phyloseq) library(DECIPHER) library(phangorn) # ----------- Set path where fastq files are located ----------- path = &quot;~/project/domain/raw_data&quot; list.files(path) fnFs = sort(list.files(path, pattern=&quot;_R1_001.fastq&quot;, full.names = TRUE)) fnRs = sort(list.files(path, pattern=&quot;_R2_001.fastq&quot;, full.names = TRUE)) sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) # ----------- Inspect quality ----------- plotQualityProfile(fnFs, aggregate=TRUE) plotQualityProfile(fnRs, aggregate=TRUE) ``` ⛔# Before executing the next chunk of code inspect the generated graphs ⛔# to determine which value to use with function &quot;truncLen&quot; ```{r} # ----------- Filter and trim ----------- filtFs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_F_filt.fastq.gz&quot;)) filtRs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_R_filt.fastq.gz&quot;)) names(filtFs) = sample.names names(filtRs) = sample.names # ------ ### For Bacteria ### ------ # out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,21), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) # ------ ### For Archaea ### ------ # out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,20), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) # ------ ### For Eukaryotes ### ------ # out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(21,21), truncLen=c(280,240), maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) # ----------- Learn error rates ----------- errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE) errR = learnErrors(filtRs, multithread=TRUE, randomize=TRUE) # visualize the estimated error rates, as a sanity check if nothing else plotErrors(errF, nominalQ=TRUE) # Apply the core sample inference algorithm to the filtered and trimmed sequence data dadaFs = dada(filtFs, err=errF, pool = &quot;pseudo&quot;, multithread=TRUE) dadaRs = dada(filtRs, err=errR, pool = &quot;pseudo&quot;, multithread=TRUE) # ----------- Construct amplicon sequence variant table (ASV) ----------- # Merging the paired reads mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs) # Construct an amplicon sequence variant table (ASV) table (a higher-resolution version of the OTU table produced by traditional methods) seqtab = makeSequenceTable(mergers) #View dimension of your matrices dim(seqtab) # Inspect distribution of sequence lengths table(nchar(getSequences(seqtab))) # ----------- Remove chimeras ----------- seqtab.nochim = removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE) #View dimension of your matrices dim(seqtab.nochim) sum(seqtab.nochim)/sum(seqtab) # ----------- Track reads through pipeline ----------- getN = function(x) sum(getUniques(x)) track = cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nonchim&quot;) rownames(track) = sample.names track # ----------- Assign taxonomy ----------- # ------ ### For Procayotes ### ------ # taxa = assignTaxonomy(seqtab.nochim, &quot;/home/16S_db/silva_nr99_v138.1_train_set.fa.gz&quot;, multithread=TRUE, tryRC=TRUE) # ------ ### Further classification for Archaea ### ------ # # Extract sequences not identified to the phylum and domain taxint = subset(taxa, is.na(phylum)) taxide = subset(taxa, !(is.na(domain))) # View how many sequences dim(taxint) seqtabint = as.data.frame(seqtab.nochim) seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)] # Reclassify with custom database arc.cassandre load(&quot;/home/16S_db/arc.cassandre.trainingset.RData&quot;) dna = DNAStringSet(getSequences(seqtabint)) # Create a DNAStringSet from the ASVs ids = IdTaxa(dna, trainingSet, strand=&quot;both&quot;, processors=NULL, verbose=FALSE, threshold = 50) taxint = t(sapply(ids, function(x) { m = match(ranks, x$rank) taxa = x$taxon[m] taxa[startsWith(taxa, &quot;Unclassified_&quot;)] = NA taxa })) colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint) # Keep only sequences classified Archaea taxint = subset(as.data.frame(taxint), domain ==&quot;Archaea&quot;) # Swap previously classified sequences with SILVA to those classified using the custom and more precise database taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),] # Merge both tables taxa = rbind(taxide, as.data.frame(taxint)) seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)] # ------ ### For Eukaryote ### ------ # taxa = assignTaxonomy(seqtab.nochim, &quot;/home/16S_db/pr2_version_5.0.0_SSU_dada2.fasta.gz&quot;, multithread=TRUE, tryRC=TRUE, taxLevels = c(&quot;Kingdom&quot;,&quot;Supergroup&quot;,&quot;Division&quot;,&quot;Class&quot;,&quot;Order&quot;,&quot;Family&quot;,&quot;Genus&quot;,&quot;Species&quot;)) # ----------- Add highest classified rank to unclassified ----------- # transpose table taxid = data.frame(t(taxa)) # As a sanity check transform every cells to characters taxid[] = lapply(taxid, as.character) # Fills the NAs with the most recent non-NA value taxa2= tidyr::fill(taxid, colnames(taxid),.direction = &quot;down&quot;) # Paste Unclassified_ to the beginning of every cells taxa2= sapply(taxa2, function(x){paste0(&quot;Unclassified_&quot;, x)}) # Replace NAs to it&#39;s value from the table taxa2 taxid[is.na(taxid)] = taxa2[is.na(taxid)] # Transpose table again taxid = t(taxid) # ----------- Remove contaminants ----------- meta = read.table(&quot;metadata.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE) ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta)) contamdf.prev = isContaminant(ps, method=&quot;prevalence&quot;, neg=&quot;negative&quot;, threshold=0.5) # Get the count of TRUE contaminant vs FALSE table(contamdf.prev$contaminant) # Remove sequences identified as contaminant ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) # Remove negative control samples ps_decontam=subset_samples(ps.noncontam, !negative==&quot;TRUE&quot;) # ----------- Phylogenetic tree ----------- # Extract sequences from phyloseq object asv_tab=as.data.frame(otu_table(ps_decontam)) seqs = getSequences(t(asv_tab)) names(seqs) = seqs # Aligning sequences seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20) # Set path to save the aligned sequence fastq file writeXStringSet(seq_align, file = &quot;~/project/domain/raw_data/align.fasta&quot;,format=&quot;fasta&quot;) ``` # ----------- Generate tree using FastTree ----------- ```{bash} # Change directory to the one containing the align.fasta file cd ~/project/domain/raw_data/ # Execute fasttree algorithm on the align.fasta file and generate the file align_tree fasttree -nt -gtr align.fasta &gt; align_tree ``` # ----------- Root tree ----------- ```{r} # Load file align_tree Tree = ape::read.tree(file = &quot;~/project/domain/raw_data/align.fasta&quot;) # Add midpoint to tree Tree.midpoint = phangorn::midpoint(Tree) # ----------- Add rooted tree to phyloseq objet ----------- tree = phy_tree(Tree.midpoint) ps1 = merge_phyloseq(ps_decontam, tree) # ----------- Add DNA sequence ----------- dna = Biostrings::DNAStringSet(taxa_names(ps_decontam)) names(dna) = taxa_names(ps_decontam) ps1=merge_phyloseq(ps1, dna) # ----------- Shorten ASV name ----------- taxa_names(ps1) = paste0(&quot;ASV&quot;, seq(ntaxa(ps1))) # ----------- Save tables ----------- saving_path = &quot;~/project/domain/int_data&quot; write.csv(as.data.frame(as(tax_table(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_taxa.csv&quot;)) write.csv(as.data.frame(as(otu_table(ps1), &quot;matrix&quot;)),file = glue(&quot;{path}/raw_asv.csv&quot;)) write.csv(as.data.frame(as(sample_data(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_meta.csv&quot;)) tree.raw = phy_tree(ps1) ape::write.tree(tree.raw , file = glue(&quot;{path}/raw_tree.tree&quot;)) ps1 %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(glue(&quot;{path}/raw_refseq.fna&quot;), append=FALSE, compress=FALSE, compression_level=NA, format=&quot;fasta&quot;) ``` 2.3 Forward reads only ```{r} # ----------- Load libraries ----------- library(dada2) library(decontam) library(phyloseq) library(DECIPHER) library(phangorn) # ----------- Getting ready ----------- path = path = &quot;~/project/archaea/raw_data&quot; fnFs = sort(list.files(file.path(path,&quot;fasta&quot;),pattern=&quot;_R1_001.fastq&quot;, full.names = TRUE)) sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) # ----------- Inspect quality ----------- plotQualityProfile(fnFs, aggregate=TRUE) ``` ⛔# Before executing the next chunk of code inspect the generated graph ⛔# to determine which value to use with function &quot;truncLen&quot; ```{r} # ----------- Filter and trim ----------- filtFs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_F_filt.fastq.gz&quot;)) names(filtFs) = sample.names out = filterAndTrim(fnFs, filtFs, trimLeft = c(18), truncLen=c(210), maxN=0, maxEE=2 , truncQ=2, compress=TRUE, multithread=TRUE) # ----------- Learn error rates ----------- errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE) plotErrors(errF, nominalQ=TRUE) dadaFs = dada(filtFs, err=errF, multithread=TRUE) # ----------- Construct amplicon sequence variant table (ASV) ----------- seqtab = makeSequenceTable(dadaFs) dim(seqtab) # ----------- Remove chimeras ----------- seqtab.nochim = removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE) dim(seqtab.nochim) sum(seqtab.nochim)/sum(seqtab) # ----------- Track reads through pipeline ----------- getN = function(x) sum(getUniques(x)) track = cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim)) colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;nonchim&quot;) rownames(track) = sample.names head(track) # ----------- Assign taxonomy ----------- taxa = assignTaxonomy(seqtab.nochim, &quot;/home/16S_db/silva_nr99_v138.1_train_set.fa.gz&quot;, multithread=TRUE, tryRC=TRUE) # ----------- Reclassify with arc.cassandre ----------- taxint = subset(taxa, is.na(phylum)) taxide = subset(taxa, !(is.na(domain))) dim(taxint) seqtabint = as.data.frame(seqtab.nochim) seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)] load(&quot;/home/16S_db/arc.cassandre.trainingset.RData&quot;) dna = DNAStringSet(getSequences(seqtabint)) ids = IdTaxa(dna, trainingSet, strand=&quot;both&quot;, processors=NULL, verbose=FALSE, threshold = 50) taxint = t(sapply(ids, function(x) { m = match(ranks, x$rank) taxa = x$taxon[m] taxa[startsWith(taxa, &quot;Unclassified_&quot;)] = NA taxa })) colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint) taxint=subset(as.data.frame(taxint), domain ==&quot;Archaea&quot;) taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),] taxa = rbind(taxide, as.data.frame(taxint)) # ----------- Add highest classified rank to unclassified ----------- taxid = as.data.frame(t(taxa)) taxid[] = lapply(taxid, as.character) taxid2= tidyr::fill(taxid, names(taxid),.direction = &quot;down&quot;) taxid2= sapply(taxid2, function(x){paste0(&quot;Unclassified_&quot;, x)}) taxid[is.na(taxid)] = taxid2[is.na(taxid)] taxid = t(taxid) taxid[ taxid == &quot;Unclassified_NA&quot; ] = NA taxid =subset(as.data.frame(taxid), domain ==&quot;Archaea&quot;) seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)] # ----------- Remove contaminants ----------- meta = read.table(&quot;metadata.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE) ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta)) contamdf.prev = isContaminant(ps, method=&quot;prevalence&quot;, neg=&quot;negative&quot;, threshold=0.5) table(contamdf.prev$contaminant) # Get the count of TRUE contaminant vs FALSE ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) # Remove sequences identified as contaminant ps_decontam=subset_samples(ps.noncontam, !negative==&quot;TRUE&quot;) # Remove negative control samples # ----------- Phylogenetic tree ----------- asv_tab=as.data.frame(otu_table(ps_decontam)) seqs = getSequences(t(asv_tab)) names(seqs) = seqs seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20) path=&quot;combined_fastq&quot; writeXStringSet(seq_align, file = file.path(path,&quot;align.fasta&quot;),format=&quot;fasta&quot;) ``` # ----------- Generate tree using FastTree ----------- ```{bash} # Change directory to the one containing the align.fasta file cd ~/project/archaea/raw_data/combined_fastq fasttree -nt -gtr align.fasta &gt; align_tree ``` # ----------- Root tree ----------- ```{r} Tree = ape::read.tree(file.path(path,&quot;tree&quot;)) Tree.midpoint = phangorn::midpoint(Tree) ape::write.tree(Tree.midpoint,file = file.path(path,&quot;tree.midpoint&quot;)) # ----------- Add rooted tree to phyloseq objet ----------- tree = phy_tree(Tree.midpoint) ps1 = merge_phyloseq(ps_decontam, tree) # ----------- Add DNA sequence ----------- dna = Biostrings::DNAStringSet(taxa_names(ps_decontam)) names(dna) = taxa_names(ps_decontam) ps1=merge_phyloseq(ps1, dna) # ----------- Shorten ASV name ----------- taxa_names(ps1) = paste0(&quot;ASV&quot;, seq(ntaxa(ps1))) # ----------- Save tables ----------- saving_path = &quot;~/project/archaea/int_data&quot; write.csv(as.data.frame(as(tax_table(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_taxa.csv&quot;)) write.csv(as.data.frame(as(otu_table(ps1), &quot;matrix&quot;)),file = glue(&quot;{path}/raw_asv.csv&quot;)) write.csv(as.data.frame(as(sample_data(ps1), &quot;matrix&quot;)), file = glue(&quot;{path}/raw_meta.csv&quot;)) tree.raw = phy_tree(ps1) ape::write.tree(tree.raw , file = glue(&quot;{path}/raw_tree.tree&quot;)) ps1 %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(glue(&quot;{path}/raw_refseq.fna&quot;), append=FALSE, compress=FALSE, compression_level=NA, format=&quot;fasta&quot;) ``` "],["rarefaction.html", "3 Rarefaction 3.1 Load required libraries 3.2 Getting ready 3.3 Filter low abundant taxa 3.4 Generate rarefaction curve 3.5 Customizing the graph 3.6 Comparing richness and diversity between rarefied and un-rarefied samples 3.7 Complete code", " 3 Rarefaction pre, code {white-space:pre !important; overflow-x:auto} To control for uneven sequencing effort in amplicon sequence analyses a common approach consist of normalizing the sampling depth by the random subsampling of sequences from each sample down to the lowest but reasonable sample’s depth (it is recommended to not go below 1000 sequences). This normalization method is refereed to as rarefying. While this approach is the subject of considerable debate and statistical criticism (see the 2014 PLOS Computational Biology paper, “Waste not, want not: why rarefying microbiome data is inadmissible” by McMurdie and Holmes) and alternative methods have been developed (DESeq2, cumulative sum scaling (CSS), and more…) rarefaction is still widely used and the most common normalizing method used in the literature. 3.1 Load required libraries library(vegan) library(phyloseq) library(ggplot2) library(tidyverse) library(ggpubr) library(glue) 3.2 Getting ready Load the data previously generated by the DADA2 workflow and combine into phyloseq object # Define your path path = &quot;~/project/domain/int_data&quot; asv = read.table(file = glue(&quot;{path}/raw_asv.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE, check.names=FALSE) taxa = read.table(file = glue(&quot;{path}/raw_taxa.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE) meta = read.table(file = glue(&quot;{path}/raw_meta.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE) tree = ape::read.tree( file = glue(&quot;{path}/raw_tree.tree&quot;)) # Merge into phyloseq object ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta), phy_tree(tree)) 3.3 Filter low abundant taxa One of the reasons to filter in this way is to avoid spending much time analyzing taxa that were only rarely seen. This also turns out to be a useful filter of noise (taxa that are actually just artifacts of the data collection process) A step that should probably be considered essential for datasets constructed via heuristic OTU-clustering methods, which are notoriously prone to generating spurious taxa. Here we are removing taxa with a relative abundance less than 0.005% as recommended by Bokulich et al., 2013 # Define threshold for low abundant taxa minTotRelAbun = 5e-5 # Get total sum for each taxa x = taxa_sums(ps) # Identify taxa with a total sum greater than the defined threshold keepTaxa = (x / sum(x)) &gt; minTotRelAbun # Filter out from the phyloseq object any taxa not identified in the keepTaxa object prunedSet = prune_taxa(keepTaxa, ps) # View how many taxa were removed by sample loss_taxa=data.frame(sample_sums(prunedSet), sample_sums(ps), (sample_sums(prunedSet)-sample_sums(ps))) # Remove samples with 0 sequences ! 3.4 Generate rarefaction curve Generating rarefation curves is the most common method used to visualize the ASVs richness as a function of sample size. (number of sequences). Analyzing such graphics also helps identifying the optimal rarefying threshold. To generate the rarefaction curves we are using the custom ggrare() function which runs much faster and also ultimately allows the user to modify certain parameter of the function if necessary. ggrare &lt;- function(physeq_object, step = 10, label = NULL, color = NULL, plot = TRUE, parallel = FALSE, se = TRUE) { x &lt;- methods::as(phyloseq::otu_table(physeq_object), &quot;matrix&quot;) if (phyloseq::taxa_are_rows(physeq_object)) { x &lt;- t(x) } ## This script is adapted from vegan `rarecurve` function tot &lt;- rowSums(x) S &lt;- rowSums(x &gt; 0) nr &lt;- nrow(x) rarefun &lt;- function(i) { cat(paste(&quot;rarefying sample&quot;, rownames(x)[i]), sep = &quot;\\n&quot;) n &lt;- seq(1, tot[i], by = step) if (n[length(n)] != tot[i]) { n &lt;- c(n, tot[i]) } y &lt;- vegan::rarefy(x[i, ,drop = FALSE], n, se = se) if (nrow(y) != 1) { rownames(y) &lt;- c(&quot;.S&quot;, &quot;.se&quot;) return(data.frame(t(y), Size = n, Sample = rownames(x)[i])) } else { return(data.frame(.S = y[1, ], Size = n, Sample = rownames(x)[i])) } } if (parallel) { out &lt;- parallel::mclapply(seq_len(nr), rarefun, mc.preschedule = FALSE) } else { out &lt;- lapply(seq_len(nr), rarefun) } df &lt;- do.call(rbind, out) # Get sample data if (!is.null(phyloseq::sample_data(physeq_object, FALSE))) { sdf &lt;- methods::as(phyloseq::sample_data(physeq_object), &quot;data.frame&quot;) sdf$Sample &lt;- rownames(sdf) data &lt;- merge(df, sdf, by = &quot;Sample&quot;) labels &lt;- data.frame(x = tot, y = S, Sample = rownames(x)) labels &lt;- merge(labels, sdf, by = &quot;Sample&quot;) } # Add, any custom-supplied plot-mapped variables if ( length(color) &gt; 1 ) { data$color &lt;- color names(data)[names(data) == &quot;color&quot;] &lt;- deparse(substitute(color)) color &lt;- deparse(substitute(color)) } if ( length(label) &gt; 1 ) { labels$label &lt;- label names(labels)[names(labels) == &quot;label&quot;] &lt;- deparse(substitute(label)) label &lt;- deparse(substitute(label)) } p &lt;- ggplot2::ggplot(data = data, ggplot2::aes_string(x = &quot;Size&quot;, y = &quot;.S&quot;, group = &quot;Sample&quot;, color = color)) p &lt;- p + ggplot2::labs(x = &quot;Number of sequence reads&quot;, y = &quot;ASV richness&quot;) if (!is.null(label)) { p &lt;- p + ggplot2::geom_text(data = labels, ggplot2::aes_string(x = &quot;x&quot;, y = &quot;y&quot;, label = label, color = color), size = 4, hjust = 0, show.legend=FALSE) } p &lt;- p + ggplot2::geom_line() if (se) { ## add standard error if available p &lt;- p + ggplot2::geom_ribbon(ggplot2::aes_string(ymin = &quot;.S - .se&quot;, ymax = &quot;.S + .se&quot;, color = NULL, fill = color), alpha = 0.2) } if (plot) { plot(p) } invisible(p) } Once the function is defined we can use it with the phyloseq object. If you wish to color the different curves based on a certain sample characteristics from you metadata table simply swap characteristic to the column name of interest. p = ggrare(prunedSet, step = 20, color = &quot;characteristic&quot;, label = &quot;Sample&quot;, se = FALSE) We can now either : rarefy to the lowest sequence number greater than 1000; or use the information from the graph to determine an optimal threshold. For option 1 # Get dataframe of sequences per sample sample_size = as.data.frame(sample_sums(prunedSet)) # Filter for the lowest number above 1000 rare_value = sample_size[which.max((sample_size[,1] &gt;= 1000)/sample_size[,1]),] # Rarefy to value identified as rare_value ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) # Confirm rarefaction as a sanity check sample_sums(ps_rare) For option 2 # Define value for rarefying rare_value=1019 # Rarefy to value identified as rare_value ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) # Confirm rarefaction as a sanity check sample_sums(ps_rare) 3.5 Customizing the graph We are adding a red line which corresponds to the define rarefying threshold. Again, to color your curves based on a certain sample characteristics from you metadata table simply swap characteristic to the column name of interest. Specific colors can also be defined with function scale_color_manual. p2 = p + geom_vline(xintercept=2530, color= &quot;red&quot;, linetype=&#39;dashed&#39;) + # Add vertical red line and modifying axis limits theme(text = element_text(size=12)) + theme_minimal() + labs(color=&quot;characteristic&quot;) + scale_color_manual(values = c(&quot;#72B3DA&quot;, &quot;royalblue4&quot;, &quot;#000000&quot;,&quot;#A0522D&quot;)) 3.6 Comparing richness and diversity between rarefied and un-rarefied samples # Extract ASV count matrix from both phyloseq object asv_r=data.frame(otu_table(ps_rare)) asv=data.frame(otu_table(ps)) # Remove samples deleted after rarefaction asv1=subset(asv, select=c(colnames(asv_r))) # Calculate Shannon diversity index dShannon=ChaoShannon(asv1) dShannon$rar=diversity(t(asv_r)) # Calculate species richness Sr=ChaoRichness(asv1) Sr$rar=ChaoRichness(asv_r)$Observed # ------------------ Plotting the results ------------------ gshannon=ggplot(dShannon,aes(x=Estimator, y=rar))+ stat_cor(method = &quot;spearman&quot;, digits = 4, aes(label=paste(rr.label,..p.label..,sep=&#39;~`,`~&#39;))) + geom_point()+ theme_minimal()+xlab(&quot;Non-rarefied Shannon diversity&quot;)+ylab(&quot;Rarefied Shannon diversity&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;#D6604D&quot;) + geom_abline(intercept = 0, slope = 1 , lty=2) + xlim(0,7) + ylim(0,7) + theme(text = element_text(size=12)) grichness &lt;- ggplot(Sr,aes(x=Estimator, y=rar))+ stat_cor(method = &quot;spearman&quot;, digits = 4, aes(label=paste(rr.label,..p.label..,sep=&#39;~`,`~&#39;)), label.y=4680) + geom_point()+ theme_minimal()+xlab(&quot;Non-rarefied ASV richness&quot;)+ylab(&quot;Rarefied ASV richness&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;#D6604D&quot;) + geom_abline(intercept = 0, slope = 1 , lty=2) + xlim(0,5000) + ylim(0,5000) + theme(text = element_text(size=12)) combined_plots=ggarrange(p2, ggarrange(gshannon, grichness, ncol=2, labels=c(&quot;b&quot;,&quot;c&quot;)), nrow=2, labels=&quot;a&quot;) # ------------------ Save plot ------------------ ggsave(&quot;rarefaction_curve.pdf&quot;, plot=combined_plots, width=16.5, height = 18,units=&quot;cm&quot;, path=&quot;/image/&quot;) # ------------------ Save rarefied table ------------------ write.csv(as.data.frame(as(tax_table(ps_rare), &quot;matrix&quot;)), file = glue(&quot;{path}/rarefied_taxa.csv&quot;)) write.csv(as.data.frame(as(otu_table(ps_rare), &quot;matrix&quot;)),file = glue(&quot;{path}/rarefied_asv.csv&quot;)) write.csv(as.data.frame(as(sample_data(ps_rare), &quot;matrix&quot;)), file = glue(&quot;{path}/rarefied_meta.csv&quot;)) tree.raw = phy_tree(ps_rare) ape::write.tree(tree.raw , file = glue(&quot;{path}/rarefied_tree.tree&quot;)) 3.7 Complete code You can copy-paste the following block of code inside a new markdown document. Code-chunks will be automatically generated and you can use the far right button ( ▶ ) to execute all of the code inside each chunk. ```{r} # ----------- Load libraries ----------- library(vegan) library(phyloseq) library(ggplot2) library(tidyverse) library(ggpubr) library(glue) # ----------- Define path ----------- path = &quot;~/project/domain/int_data&quot; asv = read.table(file = glue(&quot;{path}/raw_asv.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE, check.names=FALSE) taxa = read.table(file = glue(&quot;{path}/raw_taxa.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE) meta = read.table(file = glue(&quot;{path}/raw_meta.csv&quot;), sep=&quot;,&quot;, row.names=1, header=TRUE) tree = ape::read.tree( file = glue(&quot;{path}/raw_tree.tree&quot;)) # ----------- Merge into phyloseq object ----------- ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta), phy_tree(tree)) # ----------- Remove low abundant taxa ----------- # Set threshold minTotRelAbun = 5e-5 # Get total sum for each taxa x = taxa_sums(ps) # Identify taxa with a total sum greater than the defined threshold keepTaxa = (x / sum(x)) &gt; minTotRelAbun # Filter out from the phyloseq object any taxa not identified in the keepTaxa object prunedSet = prune_taxa(keepTaxa, ps) # View how many taxa were removed by sample loss_taxa=data.frame(sample_sums(prunedSet), sample_sums(ps), (sample_sums(prunedSet)-sample_sums(ps))) # ----------- Create function ----------- ggrare = function(physeq_object, step = 10, label = NULL, color = NULL, plot = TRUE, parallel = FALSE, se = TRUE) { x = methods::as(phyloseq::otu_table(physeq_object), &quot;matrix&quot;) if (phyloseq::taxa_are_rows(physeq_object)) { x &lt;- t(x) } ## This script is adapted from vegan `rarecurve` function tot &lt;- rowSums(x) S &lt;- rowSums(x &gt; 0) nr &lt;- nrow(x) rarefun &lt;- function(i) { cat(paste(&quot;rarefying sample&quot;, rownames(x)[i]), sep = &quot;\\n&quot;) n &lt;- seq(1, tot[i], by = step) if (n[length(n)] != tot[i]) { n &lt;- c(n, tot[i]) } y &lt;- vegan::rarefy(x[i, ,drop = FALSE], n, se = se) if (nrow(y) != 1) { rownames(y) &lt;- c(&quot;.S&quot;, &quot;.se&quot;) return(data.frame(t(y), Size = n, Sample = rownames(x)[i])) } else { return(data.frame(.S = y[1, ], Size = n, Sample = rownames(x)[i])) } } if (parallel) { out &lt;- parallel::mclapply(seq_len(nr), rarefun, mc.preschedule = FALSE) } else { out &lt;- lapply(seq_len(nr), rarefun) } df &lt;- do.call(rbind, out) # Get sample data if (!is.null(phyloseq::sample_data(physeq_object, FALSE))) { sdf &lt;- methods::as(phyloseq::sample_data(physeq_object), &quot;data.frame&quot;) sdf$Sample &lt;- rownames(sdf) data &lt;- merge(df, sdf, by = &quot;Sample&quot;) labels &lt;- data.frame(x = tot, y = S, Sample = rownames(x)) labels &lt;- merge(labels, sdf, by = &quot;Sample&quot;) } # Add, any custom-supplied plot-mapped variables if ( length(color) &gt; 1 ) { data$color &lt;- color names(data)[names(data) == &quot;color&quot;] &lt;- deparse(substitute(color)) color &lt;- deparse(substitute(color)) } if ( length(label) &gt; 1 ) { labels$label &lt;- label names(labels)[names(labels) == &quot;label&quot;] &lt;- deparse(substitute(label)) label &lt;- deparse(substitute(label)) } p &lt;- ggplot2::ggplot(data = data, ggplot2::aes_string(x = &quot;Size&quot;, y = &quot;.S&quot;, group = &quot;Sample&quot;, color = color)) p &lt;- p + ggplot2::labs(x = &quot;Number of sequence reads&quot;, y = &quot;ASV richness&quot;) if (!is.null(label)) { p &lt;- p + ggplot2::geom_text(data = labels, ggplot2::aes_string(x = &quot;x&quot;, y = &quot;y&quot;, label = label, color = color), size = 4, hjust = 0, show.legend=FALSE) } p &lt;- p + ggplot2::geom_line() if (se) { ## add standard error if available p &lt;- p + ggplot2::geom_ribbon(ggplot2::aes_string(ymin = &quot;.S - .se&quot;, ymax = &quot;.S + .se&quot;, color = NULL, fill = color), alpha = 0.2) } if (plot) { plot(p) } invisible(p) } # ----------- Plot rarefaction curves ----------- p = ggrare(prunedSet, step = 20, color = &quot;characteristic&quot;, label = &quot;Sample&quot;, se = FALSE) ``` OPTION 1 : rarefy to the lowest sequence number greater than 1000 ```{r} # Get dataframe of sequences per sample sample_size = as.data.frame(sample_sums(prunedSet)) # Filter for the lowest number above 1000 rare_value = sample_size[which.max((sample_size[,1] &gt;= 1000)/sample_size[,1]),] # Rarefy to value identified as rare_value ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) # Confirm rarefaction as a sanity check sample_sums(ps_rare) ``` OPTION 2 : Use the information from the graph to determine an optimal threshold ```{r} # Define value for rarefying rare_value=1019 # Rarefy to value identified as rare_value ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) # Confirm rarefaction as a sanity check sample_sums(ps_rare) ``` Save rarefied table ```{r} write.csv(as.data.frame(as(tax_table(ps_rare), &quot;matrix&quot;)), file = glue(&quot;{path}/rarefied_taxa.csv&quot;)) write.csv(as.data.frame(as(otu_table(ps_rare), &quot;matrix&quot;)),file = glue(&quot;{path}/rarefied_asv.csv&quot;)) write.csv(as.data.frame(as(sample_data(ps_rare), &quot;matrix&quot;)), file = glue(&quot;{path}/rarefied_meta.csv&quot;)) tree.raw = phy_tree(ps_rare) ape::write.tree(tree.raw , file = glue(&quot;{path}/rarefied_tree.tree&quot;)) ``` "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
