--- 
title: "Example workflow of amplicon sequence data analysis"
author: "Karine Villeneuve"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    config:
      sharing: null
    css: toc.css
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Introduction

This guide is intended to walk the user through the typical R workflow for processing raw amplicon sequencing data from paired end Illumina Miseq data into a table of exact amplicon sequence variants (ASVs) present in each sample. Please note that many other extensive documentation and tutorial pages are available for packages used in this workflow, notably [dada2]() and [phyloseq](). Therefore, for issues or for further questions about certain functions I always recommend consulting the available R documentation and if available the relevant github issues sites for answers. 

This guide assumes the following : 

(1) The user has access to one of the server from the Lazar lab (Orion, Hercules, Ulysse). Please see section [Remote login] for more details. 
(2) The user has an active VPN access. 
(3) The paired-end fastq files from Illumina Miseq sequencing were transferred to the user's home directory of her/his server. 
(4) The user has followed the `Introduction to linux` guide and is comfortable with basic command line functions such as : 

    - listing files inside a current directory (`ls`) ;
    - moving from one directory to the other (`cd`) ; 
    - creating new directory (`mkdir`) ; and 
    - moving / copying (`mv`/`cp`) files from one directory to the other. 

## Setting up your environment

Keeping your files organized is a skill that has a high long-term payoff. As you are in the thick of an analysis, you may underestimate how many files/folders you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped, which can ultimately lead to significantly slower research progress. Furthermore, one must keep in mind that someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. 

While there’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution, below we propose a simple organizational scheme which is project-oriented, maintainable and ultimately follows consistent patterns for amplicon sequence analysis. Please note that the proposed workflow assumes such organization. 

```
┌─ ~ -------------------------------- Your home directory
│   ├── chapter1_16S_diversity ------ Project with a short but meaningfull name with  
│       │                             a second level domain-specific organization (if applicable). 
│       ├── archaea 
│       ├── eukaryotes
│       ├── bacteria ---------------- Third level domain-specific organization :
│           ├── data   -------------- final datasets generated by scripts to be used for interpretation ;
│           ├── int_data  ----------- intermediate data to be used by other script (DADA2 output, rarefied data, etc.) ;
│           ├── figures  ------------ figures generated by scripts ; 
│           ├── raw_data   ---------- raw files (i.e. fastq files generated by sequencing, raw metadata table, etc.) ; and
│                 ├── BAC_sample-1_R1.fastq.gz
│                 ├── BAC_sample-1_R2.fastq.gz
│                 ├── BAC_sample-2_R1.fastq.gz
│                 ├── BAC_sample-2_R2.fastq.gz
│                 └── ...
│           └── scripts   ----------- executable and scripts. 
│                  ├── bac_DADA2.rmd
│                  ├── bac_rarefy.rmd
│                  ├── bac_alpha_div.rmd
│                  ├── bac_stacked_barchart.rmd
│                  └── ... 
│       ├── data   ------------------ Project-specific final datasets for publication
│       ├── figures ----------------- Project-specific final figures for publication
│       └── scripts ----------------- Project-specific scripts (only if applicable, i.e. multi-domain, explanatory variables, etc.)
└──────────────────────────────────────────────────────────────────
``` 

Further reading about organizing files and foldes : 

- [Organizing your project](https://jhudatascience.org/Reproducibility_in_Cancer_Informatics/organizing-your-project.html) by the Johns Hopkins Data Science Lab

- [A Quick Guide to Organizing Computational Biology Projects](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424#s9) by William Stafford Noble, 2009

- [Reddit post](https://www.reddit.com/r/bioinformatics/comments/3rdlhf/help_how_do_you_organize_your_projectsfiles/)

- [Organizing your data](https://www.mdc-berlin.de/research-data-management/organizing-your-data) by The Max Delbrück Center 

## Download fastqs files from Illumina BaseSpace Sequence Hub 

1. Open the link found in the email sent by Geneviève Bourret. 

    If this is your first time downloading your fastqs create a new BaseSpace Sequence Hub account using the email address to         which the email from Geneviève was addressed (normally this would be your UQAM's email)

2. On the pop-up window informing you that the CERMO-FC has shared the following item with you click `ACCEPT`. 

3. Click on the `PROJECTS` tab in the upper section of the page. 

4. Select your project and then click on the second round logo from the left which looks like a blank page and in the drop-down menu select `DOWNLOAD` then `PROJECT`. 

5. If required, download the Illumina Basespace downloader by clicking `INSTALL DOWNLOAD` and follow the instructions. Otherwise simply click `DOWNLOAD` to begin downloading your fastqs. 

6. Once the download is complete you will find inside the folder a folder for each of your sample inside which the forward and reverse read are both found in another folder. Instead of going into each folder individually and copying the fastqs manually we can use the terminal to do the job for us. From a new local terminal window navigate to the folder containing all the folders and execute the following command after having modified `/path/to/directory/where/to/move/fastqs` to the actual path where you wish to move your fastqs. 

```{bash, eval=FALSE}
find ./ -name "*.gz" -exec cp -prv "{}" "/path/to/directory/where/to/move/fastqs" ";"
```

7. Finally you can transfer your fastqs to your assigned server using any File Transfer Protocol (FTP) clients (such as [FileZilla](https://filezilla-project.org/) or [Cyberduck](https://cyberduck.io/)) or using the SCP (secure copy) command-line utility. 

    For SCP you can copy an entire folder by opening a new local terminal window and navigating to the directory containing the folder with the fastqs. From that directory execute the following command. You will then be asked to enter the password for your  user on the server. 

```{bash, eval=FALSE}
scp -r name_of_foler_with_fastqs username@server.bio.uqam.ca:/path/to/copy/folder
```

<!--chapter:end:index.Rmd-->

# Basic R Markdown

<!-- Chunk to allow horizontal scroll in chunks rather than wrap text  -->
```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

Ce petit guide à été rédigé afin de familiariser les nouvelles étudiantes/étudiants et stagiaires du laboratoire Lazar à l'analyse de données de séquençage d'amplicons avec R. 

## Introduction à Rstudio et Rmarkdown

Pour des raisons pratiques, je recommande de rédiger les scripts dans des fichiers de type R Markdown plutôt que R Script. Ce type de fichier permet de facilement annoter un script entre des sections de codes comprises dans un bloc (*chunk* en anglais). De plus, les blocs de codes permettent d'exécuter seulement certaines sections de code à la fois, ce qui ultimement permet de mofidier puis exécuter seulement ces blocs sans avoir à re-exécuter l'entièreté du script. 

La première étape consiste donc à créer un nouveau document de type RMardown. Pour ce fichier, ouvrez RStudio, puis cliquez sur `Fichier`. Dans le menu déroulant sélectionner `Nouveau Fichier` puis `R Markdown...`. 

Dans la nouvelle fenêtre vous pouvez donner le titre que vous voulez à votre nouveau document. 
Par défault le nouveau document affiche une petite introduction ainsi que des exemples tel que sur l'image ci-dessous : 

```{r echo=FALSE, out.width = "100%", fig.align = "center", out.lenght = "100%"}
knitr::include_graphics("data_figures/rmarkdown.png")
```

Ces informations ne sont pas pertinentes et vous pouvez supprimer l'ensemble du texte sous l'entête (l'entête correspond à la section délimitée par les trois tirets `---`).

### Bloc de codes 

Dans Rmarkdown, les lignes de codes à exécuter doivent être comprises dans un bloc/*chunk* de code. Le texte non compris dans un bloc n'est donc pas considéré comme du code, ce qui permet d'annoter minutieusement votre script entre les blocs afin de vous y retrouver facilement. 

Un bloc de code R doit toujours débuter avec les caractères suivants : ` ```{r}` 
et se terminer avec les caractères suivant : ` ``` `. Un bloc de code ressemble donc à ceci : 

````{verbatim, lang = "python"}
```{r}

``` 
```` 

Un code/*chunk* peut être inséré avec l'une des façons suivantes : 

- le raccourcit clavier : `Ctrl` + `Alt` + `I` 
- tapper les caractères délimitants (` ```{r} `  ` ``` ` )
- l'onglet `Code` puis `Insert chunk` 
- le bouton vert avec le petit c et signe de plus en haut à droite.

Un fois votre code rédigé dans le *chunk*, vous pouvez exécuter l'entièreté du code contenu dans ce chunk en appuyant sur le bouton vert en haut à droite du code (<font color='green'> ▶ </font>). 

Il est possible d'insérer des blocs de code de différents languages de programmation tels que Bash et Python, il suffit de remplacer le `r` entre les accolades par le nom du programme utilisé. Plusieurs autres options peuvent être appliqués sur les blocs, pour plus d'informations je vous recommande de consulter la documentation disponible sur internet. 

### Commandes de bases 

**Définir le répertoire de travail** 

Le répertoire/dossier de travail dans lequel se trouve les fichiers que nous voulons analyser peut être définit par défault en ultisant la commande `root.dir` dans un bloc de code de type setup tel que celui présenté ci-dessous :  

````{verbatim, lang = "python"}
```{r setup} 
knitr::opts_knit$set(root.dir="/chemin/vers/le/répertoire/")
```
```` 
Une fois le repertoire de travail définit par défaut de la sorte il n'est pas nécessaire d'inclure le chemin vers les fichiers que l'on veut importer tant qu'il se trouve dans le répertoire spécifié. Cela s'applique aussi à la sauvegarde de tableaux, figures ou autres. 

**Importer des données** 

Pour importer un fichier en format *comma seperated value* (csv) dans R on utilise la commande `read.csv` avec les arguments suivants : 

- `file` : Spécifier le nom du fichier 
- `header` : Déclarer la première ligne comme nom de colonne
- `row.names` : Déclarer la première colone comme nom de rangé
- `sep` : Déclarer la virgule comme séparateur
- `check.name` : Ne pas systématiquement remplacer le trait d'union par un point 

````{verbatim, lang = "python"}
 ```{r}
df = read.csv(file = "nom_fichier.csv", header = TRUE, row.names = 1, sep = ",", check.names = FALSE)  
```
```` 
Pour un tableau en format *tab delimited* on peut utiliser la fonction `read.table` et spécifier `\t` comme séparateur. 

**Option avancé**

Une seconde option que je recommande et utilise personnellement consiste à utiliser la fonction `glue` du package `glue` 
afin de coller le chemin prédifinit sur le nom du fichier. Dans ce cas-ci, il ne serait pas nécessaire de définir un répertoire par défaut.  
Exemple d'utilisation :  

````{verbatim, lang = "python"}
 ```{r}
# Importer le package glue 
library(glue) 
 
# Définir le chemin par la variable path
path = "/chemin/vers/le/répertoire"

# Utiliser la fonction glue et indiquer le nom de la variable utilisée entre les accolades
df = read.csv(file = glue("{path}/nom_fichier.csv"), header = TRUE, row.names = 1, sep = ",", check.names = FALSE)

# Pour enregistrer un tableau 
write.csv(x = df, file = (glue("{path}/tableau.csv")))
```
```` 

Cette option est particulièrement utile lorsque l'on veut importer des fichiers à partir d'un certain répertoire mais enregistrer les figures et tableaux produits dans un différent répertoire. On pourrait ainsi spécifier deux paths distincts. Par exemple : 

````{verbatim, lang = "python"}
 ```{r}
library(glue) 

# Définir les variable paths  
path_importer = "/chemin/vers/le/répertoire"
path_sauvegarder = "/chemin/vers/le/répertoire"

# Importer un fichier 
df = read.csv(file = glue("{path_importer}/nom_fichier.csv"), header = TRUE, row.names = 1, sep = ",", check.names = FALSE)

# Sauvegarder un tableau 
write.csv(x = df, file = (glue("{path_sauvegarder}/tableau.csv")))
```
```` 



<!--chapter:end:01_basic_R.Rmd-->


# DADA2 pipeline
<!-- Chunk to allow horizontal scroll in chunks rather than wrap text  -->
```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

This general workflow presents the typical commands used to process amplicon sequencing data. Please note that for some steps the commands will vary based on which Kingdom (Bacteria, Archaea or Eukaryote) the data being processed belongs to. 

The first section ([General workflow]) generally describes and breakdowns each steps of the analysis. In the second section ([Complete code]) the reader will find a single chunk of code which she/he can copy-paste into a new rmarkdown document and execute each chunk of code. Finally, the third section ([Forward reads only]) also contains a single chunk of code to use only with archaeal sequences when the quality of the reverse read for is of too poor quality to allow the merging of forward and reverse read and thus only the forward reads are processed. 

## General workflow 

### Getting ready 

Load required libraries
```{r, eval=FALSE}
library(dada2)
library(decontam)
library(phyloseq)
library(DECIPHER)
library(phangorn)
```

The first step is to define where the fastq files are located. **Please Modify this path accordingly**. For more information on how to organize your files and folders on your server please see section [Setting up your environment]. 

To validate that we are in the correct folder we then use the command `list.files` to print out all files contained in the folder previously defined. 
```{r, eval=FALSE}
path = "~/project/domain/raw_data"
list.files(path)
``` 
We then extract each sample name from the forward and reverse fastq files using some string manipulation assuming the name of our files respect the following format : 

- Forward reads : `sample-name_domain_xxx_L001_R1_001.fastq`
- Reverse reads : `sample-name_domain_xxx_L001_R2_001.fastq`

```{r, eval=FALSE}
fnFs = sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE)) 
fnRs = sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

sample.names = sapply(strsplit(basename(fnFs), "_"), `[`, 1)
``` 

### Inspect quality

In order to inspect the read quality profiles we use the command `plotQualityProfile` to plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq file. 
```{r, eval=FALSE}
plotQualityProfile(fnFs, aggregate=TRUE)
plotQualityProfile(fnRs, aggregate=TRUE)
``` 
In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line). The reverse reads are generally of significantly worse quality, especially at the end, which is common in Illumina sequencing.

### Filter and trim sequences 

Before trimming we assign the filenames for the filtered fastq.gz files and place filtered files in the created `filtered` subdirectory. 

The command `trimleft` is used to remove the primers (based on primer length) and `truncLen`to trim the reads based on where the average quality begins to crash on the previously generated graphs. Nucleotides after the specified position will be removed. 

**For Bacterias**

- Lenght of primer B341F (CCT ACG GGA GGC AGC AG) : 18 nucleotides
- Lenght of primer B785R  (GAC TAC HVG GGT ATC TAA TCC): 21 nucleotides

**For Archaea**

- Lenght of primer A340F (CCC TAC GGG GYG CAS CAG) : 18 nucleotides
- Lenght of primer A915R  (GTG CTC CCC CGC CAA TTC CT) : 20 nucleotides

**For Eukaryotes**

- Lenght of primer E960F (GGC TTA ATT TGA CTC AAC RCG) : 21 nucleotides
- Lenght of primer NSR1438R  (GGC TTA ATT TGA CTC AAC RCG) : 21 nucleotides

```{r, eval=FALSE}
filtFs = file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs = file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) = sample.names
names(filtRs) = sample.names
# For Bacteria 
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,21), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 
# For Archaea 
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,20), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 
# For Eukaryotes 
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(21,21), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 
``` 

### Learn error rates 

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

**Please note** that for some reason `multithread=TRUE` causes problem for users on a Microsoft Windows operating system and should therefore set this parameter to FALSE. Such modification should be made for all other instances where parameter `multithread` is specified

```{r, eval=FALSE}
# Learn error rates for forward and reverse reads
errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
errR = learnErrors(filtRs, multithread=TRUE, randomize=TRUE)

# visualize the estimated error rates, as a sanity check if nothing else
plotErrors(errF, nominalQ=TRUE)

# Apply the core sample inference algorithm to the filtered and trimmed sequence data
dadaFs = dada(filtFs, err=errF, pool = "pseudo", multithread=TRUE)
dadaRs = dada(filtRs, err=errR, pool = "pseudo", multithread=TRUE)
```

### Merge paired reads

We can now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).
```{r, eval=FALSE} 
# Merging the paired reads
mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs)
# Construct an amplicon sequence variant table (ASV) table (a higher-resolution version of the OTU table produced by traditional methods)
seqtab = makeSequenceTable(mergers)
#View dimension of your matrices 
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
``` 

#### Remove chimeras

Removing chimeras with the function `removeBimeraDenovo`. The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.
```{r, eval=FALSE}
# Remove chimeras
seqtab.nochim = removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)
#View dimension of your matrices and proportion of non-chimeric sequences
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```

### Track reads through pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline with the following commands. This is a great place to do a last sanity check. Outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the `truncLen` parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.
 
```{r, eval=FALSE}
getN = function(x) sum(getUniques(x))
track = cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) = sample.names
track
``` 

### Classify sequences

The DADA2 package provides a native implementation of the [naïve Bayesian classifier method](https://pubmed.ncbi.nlm.nih.gov/17586664/) to assign taxonomy to the sequence variants. The `assignTaxonomy` function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments. The DADA2 team maintains [DADA2-formatted reference fastas](https://benjjneb.github.io/dada2/training.html) for the three most common 16S databases (Silva, RDP and GreenGenes) as well as additional trainings fastas suitable for protists and certain contributed specific environments 

The minimum bootstrap confidence for assigning a taxonomic level.

**Database for Procaryotes**
```{r, eval=FALSE}
taxa = assignTaxonomy(seqtab.nochim, "/home/16S_db/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE, tryRC=TRUE)
``` 

**Further classification for Archaeas**

The same silva database is used for the classification of Bacterias and Archaeas but we also use a custom database to further classify sequences which failed initial classification for Archaeas. For this we use the function `IdTaxa` from the [DECIPHER](http://www2.decipher.codes/) package.

```{r, eval=FALSE}
# Extract sequences not identified to the phylum and domain
taxint = subset(taxa, is.na(phylum))
taxide = subset(taxa, !(is.na(domain)))
# View how many sequences 
dim(taxint)

seqtabint =as.data.frame(seqtab.nochim)
seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)]

# Reclassify with custom database arc.cassandre
load("/home/16S_db/arc.cassandre.trainingset.RData") 
dna = DNAStringSet(getSequences(seqtabint)) # Create a DNAStringSet from the ASVs
ids = IdTaxa(dna, trainingSet, strand="both", processors=NULL, verbose=FALSE, threshold = 50)

taxint = t(sapply(ids, function(x) {
        m = match(ranks, x$rank)
        taxa = x$taxon[m]
        taxa[startsWith(taxa, "Unclassified_")] = NA
        taxa
}))
colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint)

# Keep only sequences classified Archaea
taxint =subset(as.data.frame(taxint), domain =="Archaea")
# Swap previously classified sequences with SILVA to those classified using the custom and more precise database 
taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),]
# Merge both tables 
taxa = rbind(taxide, as.data.frame(taxint))
seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)]
```

**Database for Eukaryotes**

As of Silva version 138, the official DADA2-formatted reference fastas are optimized for classification of Bacteria and Archaea, and are not suitable for classifying Eukaryotes and we therefore use PR2 database. Note that this database has different `taxLevels` than the DADA2 default.

```{r, eval=FALSE}
taxa = assignTaxonomy(seqtab.nochim, "/home/16S_db/pr2_version_5.0.0_SSU_dada2.fasta.gz", multithread=TRUE, tryRC=TRUE, taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"))
``` 

### Add highest identified taxonomic rank to unclassified ranks 
```{r, eval=FALSE}
# transpose table 
taxid=data.frame(t(taxa)) 
# As a sanity check transform every cells to characters
taxid[] = lapply(taxid, as.character) 
# Fills the NAs with the most recent non-NA value
taxa2= tidyr::fill(taxid, colnames(taxid),.direction = "down") 
# Paste Unclassified_ to the beginning of every cells
taxa2= sapply(taxa2, function(x){paste0("Unclassified_", x)}) 
# Replace NAs to it's value from the table taxa2
taxid[is.na(taxid)] = taxa2[is.na(taxid)] 
 # Transpose table again
taxid = t(taxid)
```

Finally we remove from the tax table and ASV matrix the ASVs not classified to our domain or Kingdom of interest. If applicable replace Bacteria for either Archaea or Eukaryotes. 
```{r, eval=FALSE}
taxid=subset(as.data.frame(taxid), Kingdom =="Bacteria")
seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)]
```

### Removing contaminants 

The `decontam` package provides simple statistical methods to identify and visualize contaminating DNA features, allowing them to be removed and ultimately get a more accurate picture of the sampled communities to be constructed from marker-gene data. The package was designed to work with `phyloseq` objects from the phyloseq package. 

For this tutorial, the use of the `decontam` requires the two following : 

- A table of the relative abundances of sequence features (columns) in each sample (rows). 
- A metadata table where a defined set of “negative control” (samples in which sequencing was performed on blanks without any biological sample added) are identified as TRUE in a column called `negative` while other samples are identified as FALSE. 

We first load the metadata table where negative control are clearly identified and then generate a phyloseq object combining the table of relative abundances of sequences, the taxonomy table and the metadata table 
```{r, eval=FALSE}
meta = read.table("metadata.csv", sep=",", row.names=1, header=TRUE)
ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta))
``` 

The contaminant identification method used in this workflow is the prevalence method (presence/absence across samples). In this method, the prevalence of each sequence feature in true positive samples is compared to the prevalence in negative controls to identify contaminants.  In the prevalence test there is a special value worth knowing, `threshold=0.5`, that will identify as contaminants all sequences that are are more prevalent in negative controls than in positive samples.
```{r, eval=FALSE}
contamdf.prev = isContaminant(ps, method="prevalence", neg="negative", threshold=0.5)
# Get the count of TRUE contaminant vs FALSE
table(contamdf.prev$contaminant) 
# Remove sequences identified as contaminant
ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) 
 # Remove negative control samples
ps_decontam=subset_samples(ps.noncontam, !negative=="TRUE")
```

### Generate a phylogenetic tree  

Generating a phylogenetic tree is not mandatory for the downstream analysis unless the user plans on using any phylogeny based diversity metrics such as Unifrac. 

To build such tree we start by aligning the sequencing using the `AlignSeqs` function from package `DECIPHER`. 
```{r, eval=FALSE}
# Extract sequences from phyloseq object
asv_tab=as.data.frame(otu_table(ps_decontam))
seqs = getSequences(t(asv_tab))
names(seqs) = seqs
# Aligning sequences
seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20)
# Set path to save the aligned sequence fastq file  
writeXStringSet(seq_align, file = "~/project/domain/raw_data/align.fasta",format="fasta")
``` 

We then used the aligned fastq file to generate the tree using the tool [FastTree](http://www.microbesonline.org/fasttree/). 
As this tool is not an R library but rather a linux program the following commands must be executed from a terminal/command prompt.  

```{bash, eval=FALSE}
# Change directory to the one containing the align.fasta file
cd ~/project/domain/raw_data/
# Execute fasttree algorithm on the align.fasta file and generate the file align_tree
fasttree -nt -gtr  align.fasta > align_tree
``` 

The generated phylogenetic tree needs to be rooted to calculate any phylogeny based diversity metrics. For this we use the function `midpoint` from package [phangorn](https://www.rdocumentation.org/packages/phangorn/versions/2.11.1). 
```{r, eval=FALSE}
# Load file align_tree
Tree = ape::read.tree(file = "~/project/domain/raw_data/align_tree")
# Add midpoint to tree
Tree.midpoint = phangorn::midpoint(Tree)
``` 

Finally we add the rooted tree to our phyloseq object 
```{r, eval=FALSE}
tree = phy_tree(Tree.midpoint)
ps1 = merge_phyloseq(ps_decontam, tree)
``` 

### Add DNA sequences

```{r, eval=FALSE}
dna = Biostrings::DNAStringSet(taxa_names(ps_decontam))
names(dna) = taxa_names(ps_decontam)
ps1=merge_phyloseq(ps1, dna)
```

### Shorten ASV name 

We shorten ASV name from complete sequence to ASV#. 
```{r, eval=FALSE}
taxa_names(ps1) = paste0("ASV", seq(ntaxa(ps1)))
``` 

### Save tables 
```{r, eval=FALSE}

saving_path = "~/project/domain/int_data"

write.csv(as.data.frame(as(tax_table(ps1), "matrix")), file = glue("{path}/raw_taxa.csv"))
write.csv(as.data.frame(as(otu_table(ps1), "matrix")),file = glue("{path}/raw_asv.csv"))
write.csv(as.data.frame(as(sample_data(ps1), "matrix")), file = glue("{path}/raw_meta.csv"))
tree.raw = phy_tree(ps1)
ape::write.tree(tree.raw , file = glue("{path}/raw_tree.tree"))
ps1 %>% refseq() %>% Biostrings::writeXStringSet(glue("{path}/raw_refseq.fna"), append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")
```

## Complete code 

You can copy-paste the following block of code inside a new markdown document. 
Code-chunks will be automatically generated and you can use the far right button (<font color='green'> ▶ </font>) to execute all of the code inside each chunk.

````{verbatim, lang = "python"}
```{r}
# ----------- Load libraries -----------
library(dada2)
library(decontam)
library(phyloseq)
library(DECIPHER)
library(phangorn)

# ----------- Set path where fastq files are located -----------
path = "~/project/domain/raw_data"
list.files(path)

fnFs = sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE)) 
fnRs = sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

sample.names = sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# ----------- Inspect quality -----------
plotQualityProfile(fnFs, aggregate=TRUE)
plotQualityProfile(fnRs, aggregate=TRUE)
```

⛔# Before executing the next chunk of code inspect the generated graphs 
⛔# to determine which value to use with function "truncLen" 

```{r}
# ----------- Filter and trim ----------- 
filtFs = file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs = file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) = sample.names
names(filtRs) = sample.names

# ------ ### For Bacteria ### ------ #
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,21), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 
# ------ ### For Archaea ### ------ #
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(18,20), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 
# ------ ### For Eukaryotes ### ------ #
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(21,21), truncLen=c(280,240),
                     maxN=0, maxEE=c(2,2), truncQ=2,rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) 

# ----------- Learn error rates ----------- 
errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
errR = learnErrors(filtRs, multithread=TRUE, randomize=TRUE)

# visualize the estimated error rates, as a sanity check if nothing else
plotErrors(errF, nominalQ=TRUE)

# Apply the core sample inference algorithm to the filtered and trimmed sequence data
dadaFs = dada(filtFs, err=errF, pool = "pseudo", multithread=TRUE)
dadaRs = dada(filtRs, err=errR, pool = "pseudo", multithread=TRUE)

# ----------- Construct amplicon sequence variant table (ASV) ----------- 
# Merging the paired reads
mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs)
# Construct an amplicon sequence variant table (ASV) table (a higher-resolution version of the OTU table produced by traditional methods)
seqtab = makeSequenceTable(mergers)
#View dimension of your matrices 
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

# ----------- Remove chimeras ----------- 
seqtab.nochim = removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)
#View dimension of your matrices 
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)

# ----------- Track reads through pipeline ----------- 
getN = function(x) sum(getUniques(x))
track = cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) = sample.names
track

# ----------- Assign taxonomy  -----------

# ------ ### For Procayotes ### ------ #
taxa = assignTaxonomy(seqtab.nochim, "/home/16S_db/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE, tryRC=TRUE)

# ------ ### Further classification for Archaea ### ------ #
# Extract sequences not identified to the phylum and domain
taxint = subset(taxa, is.na(phylum))
taxide = subset(taxa, !(is.na(domain)))
# View how many sequences 
dim(taxint)
seqtabint = as.data.frame(seqtab.nochim)
seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)]
# Reclassify with custom database arc.cassandre
load("/home/16S_db/arc.cassandre.trainingset.RData") 
dna = DNAStringSet(getSequences(seqtabint)) # Create a DNAStringSet from the ASVs
ids = IdTaxa(dna, trainingSet, strand="both", processors=NULL, verbose=FALSE, threshold = 50)
taxint = t(sapply(ids, function(x) {
        m = match(ranks, x$rank)
        taxa = x$taxon[m]
        taxa[startsWith(taxa, "Unclassified_")] = NA
        taxa
}))
colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint)
# Keep only sequences classified Archaea
taxint = subset(as.data.frame(taxint), domain =="Archaea")
# Swap previously classified sequences with SILVA to those classified using the custom and more precise database 
taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),]
# Merge both tables 
taxa = rbind(taxide, as.data.frame(taxint))
seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)]

# ------ ### For Eukaryote ### ------ #
taxa = assignTaxonomy(seqtab.nochim, "/home/16S_db/pr2_version_5.0.0_SSU_dada2.fasta.gz", multithread=TRUE, tryRC=TRUE, taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"))

# ----------- Add highest classified rank to unclassified -----------
# transpose table 
taxid = data.frame(t(taxa)) 
# As a sanity check transform every cells to characters
taxid[] = lapply(taxid, as.character) 
# Fills the NAs with the most recent non-NA value
taxa2= tidyr::fill(taxid, colnames(taxid),.direction = "down") 
# Paste Unclassified_ to the beginning of every cells
taxa2= sapply(taxa2, function(x){paste0("Unclassified_", x)}) 
# Replace NAs to it's value from the table taxa2
taxid[is.na(taxid)] = taxa2[is.na(taxid)] 
 # Transpose table again
taxid = t(taxid)

# ----------- Remove contaminants -----------
meta = read.table("metadata.csv", sep=",", row.names=1, header=TRUE)
ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta))
contamdf.prev = isContaminant(ps, method="prevalence", neg="negative", threshold=0.5)
# Get the count of TRUE contaminant vs FALSE
table(contamdf.prev$contaminant) 
# Remove sequences identified as contaminant
ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) 
 # Remove negative control samples
ps_decontam=subset_samples(ps.noncontam, !negative=="TRUE")

# ----------- Phylogenetic tree -----------
# Extract sequences from phyloseq object
asv_tab=as.data.frame(otu_table(ps_decontam))
seqs = getSequences(t(asv_tab))
names(seqs) = seqs
# Aligning sequences
seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20)
# Set path to save the aligned sequence fastq file  
writeXStringSet(seq_align, file = "~/project/domain/raw_data/align.fasta",format="fasta")
```

# ----------- Generate tree using FastTree -----------  

```{bash}
# Change directory to the one containing the align.fasta file
cd ~/project/domain/raw_data/
# Execute fasttree algorithm on the align.fasta file and generate the file align_tree
fasttree -nt -gtr  align.fasta > align_tree
```

# ----------- Root tree -----------

```{r}
# Load file align_tree
Tree = ape::read.tree(file = "~/project/domain/raw_data/align.fasta")
# Add midpoint to tree
Tree.midpoint = phangorn::midpoint(Tree)

# ----------- Add rooted tree to phyloseq objet -----------
tree = phy_tree(Tree.midpoint)
ps1 = merge_phyloseq(ps_decontam, tree)

# ----------- Add DNA sequence -----------
dna = Biostrings::DNAStringSet(taxa_names(ps_decontam))
names(dna) = taxa_names(ps_decontam)
ps1=merge_phyloseq(ps1, dna)

# ----------- Shorten ASV name -----------
taxa_names(ps1) = paste0("ASV", seq(ntaxa(ps1)))

# ----------- Save tables -----------
saving_path = "~/project/domain/int_data"

write.csv(as.data.frame(as(tax_table(ps1), "matrix")), file = glue("{path}/raw_taxa.csv"))
write.csv(as.data.frame(as(otu_table(ps1), "matrix")),file = glue("{path}/raw_asv.csv"))
write.csv(as.data.frame(as(sample_data(ps1), "matrix")), file = glue("{path}/raw_meta.csv"))
tree.raw = phy_tree(ps1)
ape::write.tree(tree.raw , file = glue("{path}/raw_tree.tree"))
ps1 %>% refseq() %>% Biostrings::writeXStringSet(glue("{path}/raw_refseq.fna"), append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")
```
````

## Forward reads only
````{verbatim, lang="python"}
```{r}
# ----------- Load libraries -----------
library(dada2)
library(decontam)
library(phyloseq)
library(DECIPHER)
library(phangorn)

# ----------- Getting ready -----------
path = path = "~/project/archaea/raw_data"
fnFs = sort(list.files(file.path(path,"fasta"),pattern="_R1_001.fastq", full.names = TRUE))
sample.names = sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# ----------- Inspect quality -----------
plotQualityProfile(fnFs, aggregate=TRUE)
```

⛔# Before executing the next chunk of code inspect the generated graph
⛔# to determine which value to use with function "truncLen" 

```{r}
# ----------- Filter and trim ----------- 
filtFs = file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
names(filtFs) = sample.names

out = filterAndTrim(fnFs, filtFs, trimLeft = c(18), truncLen=c(210),
                     maxN=0, maxEE=2 , truncQ=2,
                     compress=TRUE, multithread=TRUE) 

# ----------- Learn error rates ----------- 
errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE)
plotErrors(errF, nominalQ=TRUE)
dadaFs = dada(filtFs, err=errF, multithread=TRUE)

# ----------- Construct amplicon sequence variant table (ASV) ----------- 
seqtab = makeSequenceTable(dadaFs)
dim(seqtab)

# ----------- Remove chimeras ----------- 
seqtab.nochim = removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)

# ----------- Track reads through pipeline ----------- 
getN = function(x) sum(getUniques(x))
track = cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim))
colnames(track) = c("input", "filtered", "denoisedF", "nonchim")
rownames(track) = sample.names
head(track)

# ----------- Assign taxonomy  -----------
taxa = assignTaxonomy(seqtab.nochim, "/home/16S_db/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE, tryRC=TRUE)

# ----------- Reclassify with arc.cassandre -----------
taxint = subset(taxa, is.na(phylum))
taxide = subset(taxa, !(is.na(domain)))
dim(taxint)

seqtabint = as.data.frame(seqtab.nochim)
seqtabint = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxint)]

load("/home/16S_db/arc.cassandre.trainingset.RData") 
dna = DNAStringSet(getSequences(seqtabint)) 
ids = IdTaxa(dna, trainingSet, strand="both", processors=NULL, verbose=FALSE, threshold = 50)

taxint = t(sapply(ids, function(x) {
        m = match(ranks, x$rank)
        taxa = x$taxon[m]
        taxa[startsWith(taxa, "Unclassified_")] = NA
        taxa
}))
colnames(taxint) = ranks; rownames(taxint) = getSequences(seqtabint)
taxint=subset(as.data.frame(taxint), domain =="Archaea")
taxide = taxide[!(rownames(taxide) %in% rownames(taxint)),]
taxa = rbind(taxide, as.data.frame(taxint))

# ----------- Add highest classified rank to unclassified -----------
taxid = as.data.frame(t(taxa))
taxid[] = lapply(taxid, as.character)
taxid2= tidyr::fill(taxid, names(taxid),.direction = "down")
taxid2= sapply(taxid2, function(x){paste0("Unclassified_", x)})
taxid[is.na(taxid)] = taxid2[is.na(taxid)]
taxid = t(taxid)
taxid[ taxid == "Unclassified_NA" ] = NA

taxid =subset(as.data.frame(taxid), domain =="Archaea")
seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)]

# ----------- Remove contaminants -----------
meta = read.table("metadata.csv", sep=",", row.names=1, header=TRUE)
ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta))

contamdf.prev = isContaminant(ps, method="prevalence", neg="negative", threshold=0.5)
table(contamdf.prev$contaminant) # Get the count of TRUE contaminant vs FALSE
ps.noncontam = prune_taxa(!contamdf.prev$contaminant, ps) # Remove sequences identified as contaminant
ps_decontam=subset_samples(ps.noncontam, !negative=="TRUE") # Remove negative control samples

# ----------- Phylogenetic tree -----------
asv_tab=as.data.frame(otu_table(ps_decontam))
seqs = getSequences(t(asv_tab))
names(seqs) = seqs
seq_align = AlignSeqs(DNAStringSet(seqs), anchor=NA, processors=20)
path="combined_fastq"
writeXStringSet(seq_align, file = file.path(path,"align.fasta"),format="fasta")
```

# ----------- Generate tree using FastTree -----------  

```{bash}
# Change directory to the one containing the align.fasta file
cd ~/project/archaea/raw_data/combined_fastq
fasttree -nt -gtr  align.fasta > align_tree
``` 

# ----------- Root tree -----------

```{r}
Tree = ape::read.tree(file.path(path,"tree"))
Tree.midpoint = phangorn::midpoint(Tree)
ape::write.tree(Tree.midpoint,file = file.path(path,"tree.midpoint"))

# ----------- Add rooted tree to phyloseq objet -----------
tree = phy_tree(Tree.midpoint)
ps1 = merge_phyloseq(ps_decontam, tree)

# ----------- Add DNA sequence -----------
dna = Biostrings::DNAStringSet(taxa_names(ps_decontam))
names(dna) = taxa_names(ps_decontam)
ps1=merge_phyloseq(ps1, dna)


# ----------- Shorten ASV name -----------
taxa_names(ps1) = paste0("ASV", seq(ntaxa(ps1)))

# ----------- Save tables -----------
saving_path = "~/project/archaea/int_data"

write.csv(as.data.frame(as(tax_table(ps1), "matrix")), file = glue("{path}/raw_taxa.csv"))
write.csv(as.data.frame(as(otu_table(ps1), "matrix")),file = glue("{path}/raw_asv.csv"))
write.csv(as.data.frame(as(sample_data(ps1), "matrix")), file = glue("{path}/raw_meta.csv"))
tree.raw = phy_tree(ps1)
ape::write.tree(tree.raw , file = glue("{path}/raw_tree.tree"))
ps1 %>% refseq() %>% Biostrings::writeXStringSet(glue("{path}/raw_refseq.fna"), append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")

```
````


<!--chapter:end:01-DADA2.Rmd-->

# Rarefaction

<!-- Chunk to allow horizontal scroll in chunks rather than wrap text  -->
```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

To control for uneven sequencing effort in amplicon sequence analyses a common approach consist of normalizing the sampling depth by the random subsampling of sequences from each sample down to the lowest but reasonable sample’s depth (it is recommended to not go below 1000 sequences). This normalization method is refereed to as rarefying. While this approach is the subject of considerable debate and statistical criticism (see the 2014 PLOS Computational Biology paper, “Waste not, want not: why rarefying microbiome data is inadmissible” by McMurdie and Holmes) and alternative methods have been developed (DESeq2, cumulative sum scaling (CSS), and more…) rarefaction is still widely used and the most common normalizing method used in the literature.

## Load required libraries
```{r, eval=FALSE}
library(vegan)
library(phyloseq)
library(ggplot2)
library(tidyverse)
library(ggpubr)
library(glue)
```

## Getting ready

Load the data previously generated by the DADA2 workflow and combine into phyloseq object
```{r, eval=FALSE}
# Define your path 
path = "~/project/domain/int_data"

asv = read.table(file = glue("{path}/raw_asv.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE) 
taxa = read.table(file = glue("{path}/raw_taxa.csv"), sep=",", row.names=1, header=TRUE) 
meta = read.table(file = glue("{path}/raw_meta.csv"), sep=",", row.names=1, header=TRUE) 
tree = ape::read.tree( file = glue("{path}/raw_tree.tree"))

# Merge into phyloseq object
ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta), phy_tree(tree))
``` 

## Filter low abundant taxa 

One of the reasons to filter in this way is to avoid spending much time analyzing taxa that were only rarely seen. 
This also turns out to be a useful filter of noise (taxa that are actually just artifacts of the data collection process)
A step that should probably be considered essential for datasets constructed via heuristic OTU-clustering methods, which are notoriously prone to generating spurious taxa. Here we are removing taxa with a relative abundance less than 0.005% as recommended by Bokulich et al., 2013

```{r, eval=FALSE}
# Define threshold for low abundant taxa
minTotRelAbun = 5e-5
# Get total sum for each taxa
x = taxa_sums(ps)
# Identify taxa with a total sum greater than the defined threshold
keepTaxa = (x / sum(x)) > minTotRelAbun
# Filter out from the phyloseq object any taxa not identified in the keepTaxa object
prunedSet = prune_taxa(keepTaxa, ps)
# View how many taxa were removed by sample 
loss_taxa=data.frame(sample_sums(prunedSet), sample_sums(ps), (sample_sums(prunedSet)-sample_sums(ps)))

# Remove samples with 0 sequences ! 
```

## Generate rarefaction curve 

Generating rarefation curves is the most common method used to visualize the ASVs richness as a function of sample size. (number of sequences). Analyzing such graphics also helps identifying the optimal rarefying threshold. 

To generate the rarefaction curves we are using the custom ggrare() function which runs much faster and also ultimately allows the user to modify certain parameter of the function if necessary.  
```{r, eval=FALSE}
ggrare <- function(physeq_object, step = 10, label = NULL, color = NULL, plot = TRUE, parallel = FALSE, se = TRUE) {
  
  x <- methods::as(phyloseq::otu_table(physeq_object), "matrix")
  if (phyloseq::taxa_are_rows(physeq_object)) { x <- t(x) }
  
  ## This script is adapted from vegan `rarecurve` function
  tot <- rowSums(x)
  S <- rowSums(x > 0)
  nr <- nrow(x)
  
  rarefun <- function(i) {
    cat(paste("rarefying sample", rownames(x)[i]), sep = "\n")
    n <- seq(1, tot[i], by = step)
    if (n[length(n)] != tot[i]) {
      n <- c(n, tot[i])
    }
    y <- vegan::rarefy(x[i, ,drop = FALSE], n, se = se)
    if (nrow(y) != 1) {
      rownames(y) <- c(".S", ".se")
      return(data.frame(t(y), Size = n, Sample = rownames(x)[i]))
    } else {
      return(data.frame(.S = y[1, ], Size = n, Sample = rownames(x)[i]))
    }
  }
  if (parallel) {
    out <- parallel::mclapply(seq_len(nr), rarefun, mc.preschedule = FALSE)
  } else {
    out <- lapply(seq_len(nr), rarefun)
  }
  df <- do.call(rbind, out)
  
  # Get sample data
  if (!is.null(phyloseq::sample_data(physeq_object, FALSE))) {
    sdf <- methods::as(phyloseq::sample_data(physeq_object), "data.frame")
    sdf$Sample <- rownames(sdf)
    data <- merge(df, sdf, by = "Sample")
    labels <- data.frame(x = tot, y = S, Sample = rownames(x))
    labels <- merge(labels, sdf, by = "Sample")
  }
  
  # Add, any custom-supplied plot-mapped variables
  if ( length(color) > 1 ) {
    data$color <- color
    names(data)[names(data) == "color"] <- deparse(substitute(color))
    color <- deparse(substitute(color))
  }
  
  if ( length(label) > 1 ) {
    labels$label <- label
    names(labels)[names(labels) == "label"] <- deparse(substitute(label))
    label <- deparse(substitute(label))
  }
  
  p <- ggplot2::ggplot(data = data,
                       ggplot2::aes_string(x = "Size",
                                           y = ".S",
                                           group = "Sample",
                                           color = color))
  
  p <- p + ggplot2::labs(x = "Number of sequence reads", y = "ASV richness")
  
  if (!is.null(label)) {
    p <- p + ggplot2::geom_text(data = labels,
                                ggplot2::aes_string(x = "x",
                                                    y = "y",
                                                    label = label,
                                                    color = color),
                                size = 4, hjust = 0, show.legend=FALSE)
  }
  
  p <- p + ggplot2::geom_line()
  if (se) { ## add standard error if available
    p <- p +
      ggplot2::geom_ribbon(ggplot2::aes_string(ymin = ".S - .se",
                                               ymax = ".S + .se",
                                               color = NULL,
                                               fill = color),
                           alpha = 0.2)
  }
  if (plot) {
    plot(p)
  }
  invisible(p)
}
``` 

Once the function is defined we can use it with the phyloseq object. If you wish to color the different curves based on a certain sample characteristics from you metadata table simply swap `characteristic` to the column name of interest. 
```{r, eval=FALSE}
p = ggrare(prunedSet, step = 20, color = "characteristic", label = "Sample", se = FALSE) 
``` 

We can now either : 

(1) rarefy to the lowest sequence number greater than 1000; 
(2) or use the information from the graph to determine an optimal threshold. 

For option 1 
```{r, eval=FALSE}
# Get dataframe of sequences per sample
sample_size = as.data.frame(sample_sums(prunedSet))
# Filter for the lowest number above 1000 
rare_value = sample_size[which.max((sample_size[,1] >= 1000)/sample_size[,1]),]
# Rarefy to value identified as rare_value
ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) 
# Confirm rarefaction as a sanity check 
sample_sums(ps_rare)
```

For option 2 
```{r,eval=FALSE}
# Define value for rarefying
rare_value=1019 
# Rarefy to value identified as rare_value
ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) 
# Confirm rarefaction as a sanity check 
sample_sums(ps_rare)
``` 

## Customizing the graph

We are adding a red line which corresponds to the define rarefying threshold. Again, to color your curves based on a certain sample characteristics from you metadata table simply swap `characteristic` to the column name of interest. Specific colors can also be defined with function `scale_color_manual`. 
```{r, eval=FALSE}
p2 = p + geom_vline(xintercept=2530, color= "red", linetype='dashed') + # Add vertical red line and modifying axis limits
  theme(text = element_text(size=12)) + 
  theme_minimal() +
  labs(color="characteristic") +
  scale_color_manual(values = c("#72B3DA", "royalblue4", "#000000","#A0522D"))
``` 

## Comparing richness and diversity between rarefied and un-rarefied samples 
```{r, eval=FALSE} 
# Extract ASV count matrix from both phyloseq object 
asv_r=data.frame(otu_table(ps_rare))
asv=data.frame(otu_table(ps))
# Remove samples deleted after rarefaction
asv1=subset(asv, select=c(colnames(asv_r)))

# Calculate Shannon diversity index 
dShannon=ChaoShannon(asv1)
dShannon$rar=diversity(t(asv_r))
# Calculate species richness 
Sr=ChaoRichness(asv1)
Sr$rar=ChaoRichness(asv_r)$Observed

# ------------------ Plotting the results  ------------------ 

gshannon=ggplot(dShannon,aes(x=Estimator, y=rar))+
  stat_cor(method = "spearman", digits = 4, aes(label=paste(rr.label,..p.label..,sep='~`,`~'))) +
  geom_point()+
  theme_minimal()+xlab("Non-rarefied Shannon diversity")+ylab("Rarefied Shannon diversity") +
  geom_smooth(method = "lm", se = F, color = "#D6604D") +
  geom_abline(intercept = 0, slope = 1 , lty=2) +
  xlim(0,7) + ylim(0,7) +
  theme(text = element_text(size=12))


grichness <- ggplot(Sr,aes(x=Estimator, y=rar))+
  stat_cor(method = "spearman", digits = 4, aes(label=paste(rr.label,..p.label..,sep='~`,`~')), label.y=4680) +
  geom_point()+
  theme_minimal()+xlab("Non-rarefied ASV richness")+ylab("Rarefied ASV richness") +
  geom_smooth(method = "lm", se = F, color = "#D6604D") +
  geom_abline(intercept = 0, slope = 1 , lty=2) +
  xlim(0,5000) + ylim(0,5000) + 
  theme(text = element_text(size=12))


combined_plots=ggarrange(p2, ggarrange(gshannon, grichness, ncol=2, labels=c("b","c")), nrow=2, labels="a")


# ------------------ Save plot  ------------------ 

ggsave("rarefaction_curve.pdf", plot=combined_plots, width=16.5, height = 18,units="cm", path="/image/")


# ------------------ Save rarefied table  ------------------ 

write.csv(as.data.frame(as(tax_table(ps_rare), "matrix")), file = glue("{path}/rarefied_taxa.csv"))
write.csv(as.data.frame(as(otu_table(ps_rare), "matrix")),file = glue("{path}/rarefied_asv.csv"))
write.csv(as.data.frame(as(sample_data(ps_rare), "matrix")), file = glue("{path}/rarefied_meta.csv"))
tree.raw = phy_tree(ps_rare)
ape::write.tree(tree.raw , file = glue("{path}/rarefied_tree.tree"))
```


## Complete code 

You can copy-paste the following block of code inside a new markdown document. 
Code-chunks will be automatically generated and you can use the far right button (<font color='green'> ▶ </font>) to execute all of the code inside each chunk.

````{verbatim, lang = "python"}
```{r}
# ----------- Load libraries -----------
library(vegan)
library(phyloseq)
library(ggplot2)
library(tidyverse)
library(ggpubr)
library(glue)

# ----------- Define path -----------
path = "~/project/domain/int_data"

asv = read.table(file = glue("{path}/raw_asv.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE) 
taxa = read.table(file = glue("{path}/raw_taxa.csv"), sep=",", row.names=1, header=TRUE) 
meta = read.table(file = glue("{path}/raw_meta.csv"), sep=",", row.names=1, header=TRUE) 
tree = ape::read.tree( file = glue("{path}/raw_tree.tree"))

# ----------- Merge into phyloseq object -----------
ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta), phy_tree(tree))

# ----------- Remove low abundant taxa -----------
# Set threshold
minTotRelAbun = 5e-5
# Get total sum for each taxa
x = taxa_sums(ps)
# Identify taxa with a total sum greater than the defined threshold
keepTaxa = (x / sum(x)) > minTotRelAbun
# Filter out from the phyloseq object any taxa not identified in the keepTaxa object
prunedSet = prune_taxa(keepTaxa, ps)
# View how many taxa were removed by sample 
loss_taxa=data.frame(sample_sums(prunedSet), sample_sums(ps), (sample_sums(prunedSet)-sample_sums(ps)))

# ----------- Create function -----------
ggrare = function(physeq_object, step = 10, label = NULL, color = NULL, plot = TRUE, parallel = FALSE, se = TRUE) {
  x = methods::as(phyloseq::otu_table(physeq_object), "matrix")
  if (phyloseq::taxa_are_rows(physeq_object)) { x <- t(x) }
  ## This script is adapted from vegan `rarecurve` function
  tot <- rowSums(x)
  S <- rowSums(x > 0)
  nr <- nrow(x)
  rarefun <- function(i) {
    cat(paste("rarefying sample", rownames(x)[i]), sep = "\n")
    n <- seq(1, tot[i], by = step)
    if (n[length(n)] != tot[i]) {
      n <- c(n, tot[i])
    }
    y <- vegan::rarefy(x[i, ,drop = FALSE], n, se = se)
    if (nrow(y) != 1) {
      rownames(y) <- c(".S", ".se")
      return(data.frame(t(y), Size = n, Sample = rownames(x)[i]))
    } else {
      return(data.frame(.S = y[1, ], Size = n, Sample = rownames(x)[i]))
    }
  }
  if (parallel) {
    out <- parallel::mclapply(seq_len(nr), rarefun, mc.preschedule = FALSE)
  } else {
    out <- lapply(seq_len(nr), rarefun)
  }
  df <- do.call(rbind, out)
  # Get sample data
  if (!is.null(phyloseq::sample_data(physeq_object, FALSE))) {
    sdf <- methods::as(phyloseq::sample_data(physeq_object), "data.frame")
    sdf$Sample <- rownames(sdf)
    data <- merge(df, sdf, by = "Sample")
    labels <- data.frame(x = tot, y = S, Sample = rownames(x))
    labels <- merge(labels, sdf, by = "Sample")
  }
  # Add, any custom-supplied plot-mapped variables
  if ( length(color) > 1 ) {
    data$color <- color
    names(data)[names(data) == "color"] <- deparse(substitute(color))
    color <- deparse(substitute(color))
  }
  if ( length(label) > 1 ) {
    labels$label <- label
    names(labels)[names(labels) == "label"] <- deparse(substitute(label))
    label <- deparse(substitute(label))
  }
  p <- ggplot2::ggplot(data = data,
                       ggplot2::aes_string(x = "Size",
                                           y = ".S",
                                           group = "Sample",
                                           color = color))
  p <- p + ggplot2::labs(x = "Number of sequence reads", y = "ASV richness")
  if (!is.null(label)) {
    p <- p + ggplot2::geom_text(data = labels,
                                ggplot2::aes_string(x = "x",
                                                    y = "y",
                                                    label = label,
                                                    color = color),
                                size = 4, hjust = 0, show.legend=FALSE)
  }
  p <- p + ggplot2::geom_line()
  if (se) { ## add standard error if available
    p <- p +
      ggplot2::geom_ribbon(ggplot2::aes_string(ymin = ".S - .se",
                                               ymax = ".S + .se",
                                               color = NULL,
                                               fill = color),
                           alpha = 0.2)
  }
  if (plot) {
    plot(p)
  }
  invisible(p)
}

# ----------- Plot rarefaction curves -----------
p = ggrare(prunedSet, step = 20, color = "characteristic", label = "Sample", se = FALSE) 
```
OPTION 1 : rarefy to the lowest sequence number greater than 1000
```{r}
# Get dataframe of sequences per sample
sample_size = as.data.frame(sample_sums(prunedSet))
# Filter for the lowest number above 1000 
rare_value = sample_size[which.max((sample_size[,1] >= 1000)/sample_size[,1]),]
# Rarefy to value identified as rare_value
ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) 
# Confirm rarefaction as a sanity check 
sample_sums(ps_rare)
```
OPTION 2 : Use the information from the graph to determine an optimal threshold 
```{r}
# Define value for rarefying
rare_value=1019 
# Rarefy to value identified as rare_value
ps_rare=rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) 
# Confirm rarefaction as a sanity check 
sample_sums(ps_rare)
``` 

Save rarefied table
```{r}
write.csv(as.data.frame(as(tax_table(ps_rare), "matrix")), file = glue("{path}/rarefied_taxa.csv"))
write.csv(as.data.frame(as(otu_table(ps_rare), "matrix")),file = glue("{path}/rarefied_asv.csv"))
write.csv(as.data.frame(as(sample_data(ps_rare), "matrix")), file = glue("{path}/rarefied_meta.csv"))
tree.raw = phy_tree(ps_rare)
ape::write.tree(tree.raw , file = glue("{path}/rarefied_tree.tree"))
``` 
````

<!--chapter:end:02-Rarefaction.Rmd-->

# Stacked taxonomy bar graphs 
<!-- Chunk to allow horizontal scroll in chunks rather than wrap text  -->
```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

One common way to begin analyzing your data is with stacked taxonomy bar graph. 
Theses graphs are great for looking at the distribution of different taxa across all of your samples at the same time.
Yet, keep in mind that they are best used for simpler data given that there is only about a dozen or so of different colors that are easily distinguishable by the human eye. 
Still, even tho no one likes stacked taxonomy  bar graphs, they have somehow found a way into microbial ecology, so we as humans just keep making them.  

Before beginning, it is important to consider you own data and what would be the best way to present your figures to the readers. 
In the case of the data used in this pipeline, we had **two distinct experiences**, 
our samples were either **"raw" groundwater** samples collected and filtered on the same day, or **rock pellets** and **groundwater** samples collected at **different times** from **three different bioreactors**... so lots of different plots to generate with different variables...  

In the end, we decided to present our data in the following way which we deemed the easiest to interpret by the readers : 

```{r echo=FALSE, out.width = "90%", fig.align = "center", out.lenght = "90%"}
knitr::include_graphics("data/Experience 1 - Groundwater-family.png")
```

```{r echo=FALSE, out.width = "90%", fig.align = "center", out.lenght = "90%"}
knitr::include_graphics("data/Experience 2 - Groundwater-family.png")
```

Below you will find all the lines of codes which were used to generate these figures along with a thorough description of the code to allow you to adapt this code to your own data. 
We believe this code to be attractive to others based on the following characteristics : 

- The taxonomy rank of interest is defined only once (to avoid having to find and replace all occurrence of the taxonomic rank to adapt this scrip to different ranks of interest)
- The use of a *for loop* to generate and save multiple figures based on a defined iterator ([for more details on iterations and iterators](https://r4ds.had.co.nz/iteration.html))
- The grouping of less abundant taxa in the category *Others*
- The fact that this grouping is done for every sample individually using a *for loop*
- Each taxa is assigned a specific color and this same color is used in the different figures 
- The names of the taxa are presented in italic 
- The most abundant taxa is aligned along the bottom of the chart to allow a better comparison between samples
- The legend is presented in alphabetical order with the exception of the category *Others* which figures at the top 

Finally, the last section of this guide describes further ways of manipulating and presenting your plots to generate beautiful and publishable figures. 

## Required libraries 
### Installing libraries
These libraries were installed in R version 4.3.1 
```{r, eval=FALSE, cache=TRUE}
install.packages("devtools")
devtools::install_github("andyofsmeg/ggTag")
install.packages("forcats")
install.packages("dplyr")
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.17")
BiocManager::install('phyloseq')
install.packages("randomcoloR")
install.packages("ggplot2")
install.packages("tidyr")
install.packages("stringr")
install.packages("ggtext")
install.packages("DT")
```
### Loading the libraries
```{r, message=FALSE, warning=FALSE, cache=TRUE, eval=FALSE}
library(devtools)
library(ggTag) # To extract title from plots
library(forcats) # To reorder our factors (function "fct_relevel")
library(dplyr) # Dataframe manipulation (notably function "pull")
library(tidyr) # Dataframe manipulation (function "separate")
library(phyloseq) # Very pratical library for the analysis of amplicon data 
library(randomcoloR) # Generate sets of random colors
library(ggplot2) # Generate plots 
library(stringr) # Makes working with strings as easy as possible (function "str_replace")
library(ggtext) # Allows the use of markdown text (used to make names italic)
library(glue)
```


## Required files

Three files are required for this script : 

1. Amplicon sequence variant (ASV) table (`rarefied_asv_bac.csv`)
2. Taxonomy table (`rarefied_taxa_bac.csv`)
3. Sample metadata table (`rarefied_meta_bac.csv`)

**ASV and Taxonomy table **

In our case, the first two files were generated from Illumina-sequenced paired-end fastq files using the [DADA2 pipeline v1.16](https://benjjneb.github.io/dada2/tutorial.html). 
To control for uneven sequencing effort in amplicon sequence analyses we normalized the sampling depth by the random subsampling of sequences from each sample down to the lowest but reasonable sample’s depth. 
This normalization method is refereed to as rarefying. While this approach is the subject of considerable debate and statistical criticism (see the 2014 PLOS Computational Biology paper, “[Waste not, want not: why rarefying microbiome data is inadmissible](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531)” by McMurdie and Holmes) and alternative methods have been developed ([DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html), [cumulative sum scaling (CSS)](https://bioconductor.org/packages/release/bioc/html/metagenomeSeq.html), and more...) rarefaction is still widely used and very popular in the literature. **The script used for the rarefaction of our sample is available [here](insert script).** 

**Sample metadata table**

Metadata provides the key to gaining biological insight from your data. The compiling of sample metadata is typically a step you will have started before beginning your amplicon sequence analysis. 
The metadata table typically contains descriptions of the samples, such as origin, sample type, geophysical characteristics, time point, etc... 

```{r, cache=TRUE, eval=FALSE}
path="~/project/domain/int_data"

asv=read.table(file = glue("{path}/rarefied_asv_bac.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE)
taxa=read.table(file = glue("{path}/rarefied_taxa_bac.csv"), sep=",", row.names=1, header=TRUE)
meta=read.table(file = glue("{path}/rarefied_meta_bac.csv"), sep=",", row.names=1, header=TRUE)
```

```{r, echo=FALSE, cache=TRUE, eval=FALSE}
#Remove unwanted sample and column from the sample metadata file 
meta=subset(meta, !Sample_type=='Peatbog') # Remove sample from the peatbog
meta=subset(meta, select=-c(is.neg))
meta$position_pastille[meta$Sample_type == "Groundwater"] <- NA
```

Below is a preview of these three table.

```{r, echo=FALSE}
library(glue)
path="/Users/karinevilleneuve/Library/CloudStorage/OneDrive-UQAM/Github/amplicon-analysis-guide/data"
asv=read.table(file = glue("{path}/rarefied_asv_bac.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE)
taxa=read.table(file = glue("{path}/rarefied_taxa_bac.csv"), sep=",", row.names=1, header=TRUE)
meta=read.table(file = glue("{path}/rarefied_meta_bac.csv"), sep=",", row.names=1, header=TRUE)
```

```{r, echo=FALSE, cache=TRUE}
library(DT)
datatable(head(asv[1:6,6:9]), caption="ASV table", options=list(scrollX=T, dom = 't', paging=FALSE, ordering=FALSE,
                                                            initComplete = JS("function(settings, json) {",
                                                                              "$(this.api().table().header()).css({'font-size': '12px'});",
                                                                              "}"))) %>% formatStyle(columns = colnames(.$x$data), `font-size` = '12px')

datatable(head(taxa), caption="Taxonomy table", options=list(scrollX=T, dom = 't', paging=FALSE, ordering=FALSE, 
                                                            initComplete = JS("function(settings, json) {",
                                                                              "$(this.api().table().header()).css({'font-size': '12px'});",
                                                                              "}"))) %>% formatStyle(columns = colnames(.$x$data), `font-size` = '12px')

datatable(meta, caption="Sample metadata table", options=list(dom = 'tip', pageLength=5,autoWidth = TRUE,columnDefs=list(list(width="50%", targets=0)),
                                                              initComplete = JS("function(settings, json) {",
                                                                              "$(this.api().table().header()).css({'font-size': '12px'});",
                                                                              "}"))) %>% formatStyle(columns = colnames(.$x$data), `font-size` = '12px')
``` 

## Defining taxonomic rank

This is were we define which taxonomy rank we want to show in our figures. In this example we want to display the relative abundance of taxon at the genus level. 
```{r, cache=TRUE, eval=FALSE}
taxa_rank="Genus"
```

## Prepping the metadata table 

Getting your data ready for the analysis may not be the longest and most complex part of your code (especially compared to plotting the figures...), but may be the part where you spend a lot of time thinking about your own set of data. I recommend visualizing and even drawing different sets of figures in order to help you identify which variables to consider in your own code. As a reminder... 

***

<p style="text-align: center;">“*Data Scientists spend up to 80% of the time on data cleaning and 20% of their time on actual data analysis*”</p>
<p style="text-align: right;font-size:8pt">Dasu and Johnson, 2003</p>
***

In our case, we first had to generate new columns in the sample metadata table with some crucial information (**experience #**, **time**, **bioreactor #**, and **replicate #**) which were only defined in the sample names. This first chunk of code was therefore used to extract the **"row.names**" from the `meta` dataframe into a new column named **"sample_name"**, which is then split into five new columns using the hyphen (-) as separator. The last three lines of code were used to replace certain abbreviation and words in order to improve the quality and readability of the final figures. 

```{r, cache=TRUE, eval=FALSE}
# Create new column called sample_name from the row.name
meta$sample_name=row.names(meta) 
# Separate column sample_name based on hyphen into five new columns 
meta=separate(meta, 
              col=sample_name, 
              into=c("experience", 
                     "short_sample_type", 
                     "time", 
                     "bioreactor", 
                     "replicate"), 
              sep="-") 
# reset the row.name because R 
meta$sample_name=row.names(meta) 
# Replacing abbreviation
meta$experience=gsub("E", "Experience ", meta$experience) # In the column experience, replace E with Experience 
meta$bioreactor=gsub("B0","Aquifer", meta$bioreactor) # In the column bioreactor, replace B00 with Aquifer 
meta$bioreactor=gsub("B", "Bioreactor ",meta$bioreactor) # In the column bioreactor, replace B with Bioreactor 

```

We could then begin the puzzling task of identifying which variables we would use as iterator in our *for loop* to generate the figures and how to present the data on the x axis.
If we go back to our data, we have...  

- 2 experiences ;
- 3 types of samples ;
  - Raw groundwater (identified as *Aquifer*) ;
  - Rock pellets ;
  - Groundwater ;
- 3 bioreactors ;  

... all along a temporal scale. 

We decided that we wanted each figure to represent a different **experience** and **sample type**. We would therefore have four figures :

(1) Experience 1 - Groundwater
(2) Experience 1 - Rock 
(3) Experience 2 - Groundwater 
(4) Experience 2 - Rock 

In each of these figures, we would use `ggplot2` function `facet_grid` to split the graph in different panels based on the values from the column **bioreactor** 
(either *Aquifer*, *Bioreactor 1*, *Bioreactor 2*, *Bioreactor 3*). 

Sampling time would be used for the x-axis. 

Therefore, as iterator, we created a new column called **experience_sample** combining the values from the **experience** and **sample type** column. For the x-axis, we had certain samples collected at the same time to generate different replicates so we couldn't simply use the column **time**  because then those samples would be stacked together. This was notably the case for the aquifer samples from time T00 and all the rock samples from the experience 2. We had to generate a new column which we called **time_replicate** and populated this new column by using the function `ifelse` : 

- if value from the column **bioreactor** = *Aquifer*, then populate column **time_replicate**  with string "*T00.*" (for time 00) followed by the value from the column **replicate**; and 

- if value from the column **experience** = *Experience 2* and **Sample_type** = *Rock*, then populate column **time_replicate**  with the value from the column **time** followed by the string "*R.*" (for replicate) and value from the column **replicate** ; 
- for all other cases (else), simply populate the column **time_replicate** with values from the column **time**. 

```{r, cache=TRUE, eval=FALSE}
# Creating new column for our iterator by combining experience # and sample type 
meta$experience_sample=paste(meta$experience,"-",meta$Sample_type)

# create the new column to be used for the x axis (representing both times and replicate #)
meta$time_replicate=with(meta, 
                         ifelse(bioreactor=="Aquifer", paste("T00.", meta$replicate, sep = ""),
                                ifelse(experience=="Experience 2" & Sample_type=="Rock", 
                                       paste(meta$time,paste("R.", meta$replicate), sep = "-"), meta$time)))

# Getting rid of unnecessary columns
meta=subset(meta, select=-c(position_pastille, temp, ph, do_percent, dic, doc, short_sample_type))
```

This is now the look of our sample metadata table :
```{r, echo=FALSE, cache=TRUE}
library(DT)
datatable(meta, options=list(dom = 'tip', pageLength=5,autoWidth = TRUE,columnDefs=list(list(width="50%", targets=0)),
                                                              initComplete = JS("function(settings, json) {",
                                                                              "$(this.api().table().header()).css({'font-size': '12px'});",
                                                                              "}"))) %>% 
  formatStyle(columns = colnames(.$x$data), `font-size` = '12px')
``` 

## Combine and melt dataframes
We now want to combine into one single dataframe the abundance of the different ASV, the taxonomy assigned to each ASV, and our cleaned up sample metadata dataframe. 
For this we are using the package and function `phyloseq` before melting our dataframe to what we call long format.
```{r, cache=TRUE, eval=FALSE}
# Merge into phyloseq object
ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta))
# Get abundance in %
ps_rel_abund=transform_sample_counts(ps, function(x) x/sum(x)) 
# Agglomerate taxa at taxonomic rank defined earlier 
glom=tax_glom(ps_rel_abund, taxrank = taxa_rank) 
# Melt to long format 
melted_df=psmelt(glom)
```
The look of the melted dataframe : 
```{r, echo=FALSE, warning=FALSE, cache=TRUE}
library(DT)
datatable(melted_df,rownames = FALSE, options=list(dom = 'tip', scrollX=T, pageLength=5,autoWidth = TRUE,
                             initComplete = JS("function(settings, json) {",
                                               "$(this.api().table().header()).css({'font-size': '12px'});",
                                               "}"))) %>%  formatStyle(columns = colnames(.$x$data), `font-size` = '12px')
``` 

## Identify most abundant taxa 
In this step we are using a *for loop* again in order to identify in each sample the X number of most abundant taxa. All other taxa will be renamed as *Others*. 
The X number of taxa to show in every sample can be changed to whatever the user prefers by simply modifying the value for the variable `number_of_taxa`. 
Yet, keep in mind that the most abundant taxa will most probably not be the same in each sample and therefore your legend will likely include more taxa then the number defined here.  
```{r,cache=TRUE, eval=FALSE}
# Defining the number of most abundant taxa to keep
number_of_taxa=5
# Create an empty list that we will populated with the unique taxa of each sample
list_of_all_taxonomic_rank= list() 
i = 0 
# Beginning of the for loop
for (each_sample in unique(melted_df$Sample)){
    i=i+1
    sample=subset(melted_df, Sample==each_sample) # Create a new dataframe from the iterator (sample). 
    total_abundance=aggregate(sample$Abundance, by=list(taxa_rank=sample[[taxa_rank]]), FUN=sum) # Combine together the same taxa and sum the abundances 
    top=head(total_abundance[order(total_abundance$x, decreasing= T),], n=number_of_taxa) # Sort by abundance and keep only the X number of taxa defined by variable number_of_taxa
    others_df=sample[!sample[[taxa_rank]] %in% top$taxa_rank,] # Extract in a new dataframe all taxa that are not present in the dataframe `top`
    others_list=pull(others_df, taxa_rank) # Create a list by pulling all the values from the column corresponding to the taxa_rank into a list
    sample[sample[[taxa_rank]]%in% others_list,][[taxa_rank]]="Others" # In the dataframe `sample` rename all the taxa from the list `others_list` as `Others`
    list_of_all_taxonomic_rank[[i]]=sample #save this dataframe in our list 
}
df=do.call("rbind",list_of_all_taxonomic_rank) # combine all the dataframe from the list into one dataframe
unique_taxon=data.frame(unique(df[[taxa_rank]])) # create dataframe with the unique names of taxa 
name=colnames(unique_taxon) # extract the name of the column in order to rename the column with the following line 
names(unique_taxon)[names(unique_taxon)==name]=as.character(taxa_rank) # Rename the column to the taxa rank defined earlier 
```

## Assign color and italicize taxa 

We are now generating a new set of random unique colors and assigning a specific color to each taxa from our list of most abundant taxa. We are also using the *markdown syntax* to italicize the taxa name in both the legend and dataframe. 
**If you wish to use a previously saved color palette do not run the following chunk of code and jump to section [Saving and loading color range]**
```{r, eval=FALSE, cache=TRUE, eval=FALSE}
# get the total number of unique most abundant taxa 
n=nrow(unique_taxon) 
# generate a set of X unique colors corresponding to the number of unique taxa
palette=distinctColorPalette(n) 
unique_taxon[[taxa_rank]]=factor(unique_taxon[[taxa_rank]])
names(palette)=levels(unique_taxon[[taxa_rank]]) 
# assign gray to category "Others". The same nomenclature can be use to manually change certain colors. 
palette[["Others"]]="#E1E1E1" 

#  recreate palette with markdown to italicize name and remove the underscore after Unclassified
all_names=data.frame(names(palette))
names_markdown=all_names %>%
  mutate(names.palette.=str_replace(names.palette., "(.*)","*\\1*"), # Adding asterisk at beginning and end of every taxa
         names.palette.=str_replace(names.palette., "\\*Unclassified_(.*)\\*","Unclassified *\\1*"), # Removing the asterisk for words that don't need to be italicize (Unclassified and Others)
         names.palette.=str_replace(names.palette., "\\*Others\\*", "Others"))
list_names=as.vector(names_markdown$names.palette.)
# Replace names of object
names(palette)=c(list_names)

# Making the same modification to the taxa name from the legend to the taxa names in the dataframe
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "(.*)","*\\1*")
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "\\*Unclassified_(.*)\\*","Unclassified *\\1*")
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "\\*Others\\*", "Others")

# Ordering the legend in alphabetical order
legend_raw=unique(df[[taxa_rank]]) #Extract legend as text
ordered_legend=sort(legend_raw) # order alphabetically
reordered_legend=fct_relevel(ordered_legend, "Others") # move "Others" to the beginning
final_legend=levels(reordered_legend) # Extract the levels in a new object

my_scale <- scale_fill_manual(name=as.character(taxa_rank), breaks=paste(final_legend), values = palette, na.translate=FALSE, drop=TRUE, limits = force) # Recreate color fill
```

## Saving and loading color range 

If you wish to save your current color palette use the following line of code to save it as an R data file. 
```{r, eval=FALSE}
# Create a name for saving the object which includes both the rank and number of taxa
scale_name=paste(taxa_rank,"-",number_of_taxa,"-","colors",".rds", sep="")
# Save palette using the defined name 
saveRDS(my_scale,as.character(scale_name)) 
```
If you wish to load a previously saved color palette use the following line of code. 
```{r, eval=FALSE}
my_scale=readRDS("name_of_your_palette.rds") # Load color palette
```

## Plotting your figures
Finally this is where we are generating the final figures. As you can see we are again using a *for loop* to generate multiple figures with only one chunk of code. 
As a reminder we decided to generate **four figures** based on the experience number and sample type. Hence we generated a new column called **experience_sample** which we are using here as our iterator. 
We are using the function `fct_reorder` to align along the bottom and top axis the most abundant taxa. 
```{r, cache=TRUE, eval=FALSE}
plot_list = list() # Generate empty list which we will populate with our generated figures
i = 0

for (experiences_and_sample in unique(df$experience_sample)){ 
  i = i + 1
  df_sample=subset(df, experience_sample==experiences_and_sample)
  p=ggplot(df_sample, aes(x=time_replicate, weight=Abundance, fill=fct_reorder(.data[[taxa_rank]],Abundance,.desc=FALSE))) + # .data is very important to force the evaluation of the input variables (taxonomic_rank)
  geom_bar() +
  labs(y ='Relative abundance (%)', x="Time") +
  scale_y_continuous(expand = c(0,0)) + # Remove the white space 
  theme_classic() +
  theme(text = element_text(size = 9),
        axis.title=element_text(size=8),
        legend.title=element_text(size=8),
        plot.title = element_text(hjust =0.5),
        axis.text.x = element_text(angle=70,vjust = 0.5, size=7),
        legend.position="bottom", 
        legend.text=element_markdown(size=7),
        legend.key.size = unit(0.5, 'cm'), # adjusting the size of the legend colored box
        strip.background = element_blank()) + # remove facet_grid box background 
  ggtitle(paste(experiences_and_sample)) + # Use the value from our new column as plot title. This title will then be extracted to save the figures. 
  my_scale + # Load our color palette 
  facet_grid(~bioreactor, scales="free", space="free") + # facet_grid according to bioreactor 
  guides(fill=guide_legend(nrow=6, title=taxa_rank, title.position="top", title.hjust = 0.5, reverse=FALSE)) # Adjusting the legend, notably the number of rows and position
  plot_list[[i]] = p # save plot into list 
}
# Uncomment the following line to visualize your plots in R. 
# plot_list 

# Set path for saving plots
plot_path="figures" # Set the path to where you want to save your figures
for (i in plot_list[]){
  title=extractGGTitle(i) # extract title from plot
  titlepdf=paste(taxa_rank,"-", title,".pdf", sep="") # add taxa rank before title and .pdf to the end (to save figure as pdf)
  ggsave(as.character(titlepdf),plot=i, device="pdf", path=plot_path, width=27.94, height = 21.59, units="cm") # save plot
}
```

## Export tables
Now that we have generated our nice figures, I also recommend exporting the table as a comma separated value (CSV) file. By doing so, you will be able to access the numbers behind the graph, which can be interesting data to present in the **Results** section of your article.

We are first generating a new column called **Relative_abundance** which combines all the variables (**experience_sample**, **time_replicate**, and **taxa_rank**) used in generating the figures. 
We then sum the relative abundance of similar taxa with the function `aggregate`.
```{r, eval=FALSE}
df$Relative_abundance=(paste(df$experience_sample, "_", df$time_replicate,"_", df[[taxa_rank]]))
supplementary_dataframe=aggregate(x=df$Abundance, by=list(df$Relative_abundance), FUN=sum)

# Save tables
title_df=paste(taxa_rank,"-","supplementary_dataframe",".csv", sep="") 
write.csv(supplementary_dataframe, as.character(title_df), quote=FALSE, row.names=FALSE)
```

## Complete code 

You can copy-paste the following block of code inside a new markdown document. 
Code-chunks will be automatically generated and you can use the far right button (<font color='green'> ▶ </font>) to execute all of the code inside each chunk.

````{verbatim, lang = "python"}
```{r}
# ----------- Load libraries -----------
library(devtools)
library(ggTag) # To extract title from plots
library(forcats) # To reorder our factors (function "fct_relevel")
library(dplyr) # Dataframe manipulation (notably function "pull")
library(tidyr) # Dataframe manipulation (function "separate")
library(phyloseq) # Very pratical library for the analysis of amplicon data 
library(randomcoloR) # Generate sets of random colors
library(ggplot2) # Generate plots 
library(stringr) # Makes working with strings as easy as possible (function "str_replace")
library(ggtext) # Allows the use of markdown text (used to make names italic)
library(glue)

# ----------- Define path -----------
path="~/project/domain/int_data"

asv=read.table(file = glue("{path}/rarefied_asv_bac.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE)
taxa=read.table(file = glue("{path}/rarefied_taxa_bac.csv"), sep=",", row.names=1, header=TRUE)
meta=read.table(file = glue("{path}/rarefied_meta_bac.csv"), sep=",", row.names=1, header=TRUE)

# ----------- Define rank of interest -----------
taxa_rank="Genus"

# ----------- Merge into phyloseq object -----------
ps=phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta))
# Get abundance in %
ps_rel_abund=transform_sample_counts(ps, function(x) x/sum(x)) 
# Agglomerate taxa at taxonomic rank defined earlier 
glom=tax_glom(ps_rel_abund, taxrank = taxa_rank) 
# Melt to long format 
melted_df=psmelt(glom)

# Defining the number of most abundant taxa to keep
number_of_taxa=5

# ----------- Identifying most abundant taxa per sample -----------

# Create an empty list that we will populated with the unique taxa of each sample
list_of_all_taxonomic_rank= list() 
i = 0 
# Beginning of the for loop
for (each_sample in unique(melted_df$Sample)){
    i=i+1
    sample=subset(melted_df, Sample==each_sample) # Create a new dataframe from the iterator (sample). 
    total_abundance=aggregate(sample$Abundance, by=list(taxa_rank=sample[[taxa_rank]]), FUN=sum) # Combine together the same taxa and sum the abundances 
    top=head(total_abundance[order(total_abundance$x, decreasing= T),], n=number_of_taxa) # Sort by abundance and keep only the X number of taxa defined by variable number_of_taxa
    others_df=sample[!sample[[taxa_rank]] %in% top$taxa_rank,] # Extract in a new dataframe all taxa that are not present in the dataframe `top`
    others_list=pull(others_df, taxa_rank) # Create a list by pulling all the values from the column corresponding to the taxa_rank into a list
    sample[sample[[taxa_rank]]%in% others_list,][[taxa_rank]]="Others" # In the dataframe `sample` rename all the taxa from the list `others_list` as `Others`
    list_of_all_taxonomic_rank[[i]]=sample #save this dataframe in our list 
}
df=do.call("rbind",list_of_all_taxonomic_rank) # combine all the dataframe from the list into one dataframe
unique_taxon=data.frame(unique(df[[taxa_rank]])) # create dataframe with the unique names of taxa 
name=colnames(unique_taxon) # extract the name of the column in order to rename the column with the following line 
names(unique_taxon)[names(unique_taxon)==name]=as.character(taxa_rank) # Rename the column to the taxa rank defined earlier 

# ---- Create color range and italicize the taxa name ---- 
n=nrow(unique_taxon) # get the total number of unique most abundant taxa 
palette=distinctColorPalette(n) # generate a set of X unique colors corresponding to the number of unique taxa
unique_taxon[[taxa_rank]]=factor(unique_taxon[[taxa_rank]])
names(palette)=levels(unique_taxon[[taxa_rank]]) # 
palette[["Others"]]="#E1E1E1" # assign gray to category "Others". The same nomenclature can be use to manually change certain colors. 

#  recreate palette with markdown to italicize name and remove the underscore after Unclassified
all_names=data.frame(names(palette))
names_markdown=all_names %>%
  mutate(names.palette.=str_replace(names.palette., "(.*)","*\\1*"), # Adding asterisk at beginning and end of every taxa
         names.palette.=str_replace(names.palette., "\\*Unclassified_(.*)\\*","Unclassified *\\1*"), # Remove asterisk for words that don't need to be italicize (Unclassified and Others)
         names.palette.=str_replace(names.palette., "\\*Others\\*", "Others"))
list_names=as.vector(names_markdown$names.palette.)
# Replace names of object
names(palette)=c(list_names)
# Make same modification to the taxa name from the legend to the taxa names in the dataframe
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "(.*)","*\\1*")
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "\\*Unclassified_(.*)\\*","Unclassified *\\1*")
df[[taxa_rank]]=str_replace(df[[taxa_rank]], "\\*Others\\*", "Others")
# Ordering the legend in alphabetical order
legend_raw=unique(df[[taxa_rank]]) #Extract legend as text
ordered_legend=sort(legend_raw) # order alphabetically
reordered_legend=fct_relevel(ordered_legend, "Others") # move "Others" to the beginning
final_legend=levels(reordered_legend) # Extract the levels in a new object
# Recreate color fill
my_scale <- scale_fill_manual(name=as.character(taxa_rank), breaks=paste(final_legend), values = palette, na.translate=FALSE, drop=TRUE, limits = force) 

# ---- Plot figures ---- 
plot_list = list() # Generate empty list which we will populate with our generated figures
i = 0

for (experiences_and_sample in unique(df$experience_sample)){ 
  i = i + 1
  df_sample=subset(df, experience_sample==experiences_and_sample)
  p=ggplot(df_sample, aes(x=time_replicate, weight=Abundance, fill=fct_reorder(.data[[taxa_rank]],Abundance,.desc=FALSE))) + # .data is very important to force the evaluation of the input variables (taxonomic_rank)
  geom_bar() +
  labs(y ='Relative abundance (%)', x="Time") +
  scale_y_continuous(expand = c(0,0)) + # Remove the white space 
  theme_classic() +
  theme(text = element_text(size = 9),
        axis.title=element_text(size=8),
        legend.title=element_text(size=8),
        plot.title = element_text(hjust =0.5),
        axis.text.x = element_text(angle=70,vjust = 0.5, size=7),
        legend.position="bottom", 
        legend.text=element_markdown(size=7),
        legend.key.size = unit(0.5, 'cm'), # adjusting the size of the legend colored box
        strip.background = element_blank()) + # remove facet_grid box background 
  ggtitle(paste(experiences_and_sample)) + # Use the value from our new column as plot title. This title will then be extracted to save the figures. 
  my_scale + # Load our color palette 
  facet_grid(~bioreactor, scales="free", space="free") + # facet_grid according to bioreactor 
  guides(fill=guide_legend(nrow=6, title=taxa_rank, title.position="top", title.hjust = 0.5, reverse=FALSE)) # Adjusting the legend, notably the number of rows and position
  plot_list[[i]] = p # save plot into list 
}
``` 

````

<!--chapter:end:03-stacked_taxonomy_bar_charts.Rmd-->

# PCoA 

Load package and set-up directory 
```{r, eval = FALSE}
knitr::opts_knit$set(root.dir = "~/16S_Project/chap_1_feast/DADA2_output/")
library(phyloseq)
library(vegan)
library(ggplot2)
library(ggpubr)
library(ggrepel)
library(glue)
library(cluster)
library(dplyr)
library(scales)
``` 

For this analysis we are using the same files used previously for the stacked taxonomy bar charts. 

```{r, eval = FALSE}
path="~/projet/domain/int_data"

asv = read.table(file = glue("{path}/rarefied_asv.csv"), sep=",", row.names=1, header=TRUE, check.names=FALSE) 
taxa = read.table(file = glue("{path}/rarefied_taxa.csv"), sep=",", row.names=1, header=TRUE) 
meta = read.table(file = glue("{path}/raw_meta.csv"), sep=",", row.names=1, header=TRUE) 
``` 

The first step is to apply the Hellinger data transformation on our ASV count matrix. The Hellinger standardization is a convex standardization that simultaneously helps minimize effects of vastly different sample total abundances. 

```{r, eval = FALSE}
asv_hellinger=decostand(asv, method="hellinger")
```

In case you want to use some numerical characteristics from you metadata table you'll want to transform there as factor before proceeding. For examepe, here I want to use the pore-size of my filter to distinguisgh the samples on my PCoA. Because those values are numerical, by default R will treat them as such and will display them as a gradient. To avoid this I will use the following command to transform those values from numerical to factor : 

```{r, eval = FALSE}
meta$Filter=factor(meta$Filter, levels = c("1", "2"))
```

Merge into phyloseq object 
```{r, eval = FALSE}
ps=phyloseq(otu_table(asv_hellinger, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta2))
```

Ordination
```{r, eval = FALSE}

# Calculate distance matrix 
dist=vegdist(t(asv_hellinger), method="bray")

# Ordinate distance matrix 
PCOA = cmdscale(dist, eig=TRUE, add=TRUE) # set eig to TRUE to get % of variance for each axis and correct negative eig values with add=TRUE
position=PCOA$points # Extract point position 
colnames(position)=c("Axis.1", "Axis.2") # Change column name 

percent_explained=100*PCOA$eig/sum(PCOA$eig) # get percentage of variation explained by each axis 
reduced_percent=format(round(percent_explained[1:2], digits=2), nsmall=1, trim=TRUE) # reduce number of digits

# Generate pretty labels for plot 
pretty_labs=c(glue("Axis 1 ({reduced_percent[1]}%)"), glue("Axis 2 ({reduced_percent[2]}%)")) # modify the percentage (only 2 digits after comma and show at least one digit)

df=merge(position, meta, by=0) # combine PCOA results with metadata 

```

Plot ordination

```{r, eval = FALSE}
plot=ggplot(df, aes(x=Axis.1, y=Axis.2, color=Well, shape=Filter, label=Sample)) + 
  theme_light() +
  geom_point(size=2) + 
  geom_text(hjust = 0, nudge_x = 0.01, show.legend = FALSE) + 
  labs(x=pretty_labs[1], y=pretty_labs[2], color="Well", shape="Filter", title="PCoA") + 
  theme(text = element_text(size=8), 
        plot.title=element_text(hjust=0.5, size=8)) + 
  scale_y_continuous(limits=c(-0.50,0.50)) +
  scale_x_continuous(limits=c(-0.5,0.5)) 
``` 

<!--chapter:end:04-PCoA.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-Figure_manipulation.Rmd-->

# Remote login

## RStudio Server 

RStudio Server provides a browser-based interface to a version of R running on a remote Linux server. 
To begin, open a new browser window and enter the address which corresponds to your server. Use your given credentials to log in.

- **Orion** : 132.208.64.232:8787
- **Hercules** :132.208.64.220:8787
- **Ulysse** : 132.208.64.213:8787

## Terminal 

Remotely log into your session on your attributed server by using the secure shell (ssh) cryptographic network protocol. Inside a terminal window use the following command but replace `username` and `servername` to your given credentials and server.  

```{bash, eval=FALSE} 
ssh username@servername.bio.uqam.ca 
``` 


## Usefull applications and tools

(1) the terminal application from your computer and log into your server with ssh 
(2) the RStudio `Terminal` (located next to the `Console` tab) 
(3) any File Transfer Protocol (FTP) applications such as FileZila or Cyberduck. 

<!--chapter:end:06-remote_login.Rmd-->

---
title: "PICRUSt2"
author: "Karine Villeneuve"
date: "2023-10-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

<!--chapter:end:07_PICRUSt2.Rmd-->

